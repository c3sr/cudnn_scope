#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7458384173167432510 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7458384173167432510(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7458384173167432510(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7458384173167432510(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7458384173167432510);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14416230828104774658 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14416230828104774658(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14416230828104774658(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14416230828104774658(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14416230828104774658);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4047459543683332735 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4047459543683332735(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4047459543683332735(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4047459543683332735(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4047459543683332735);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1489387103826643030 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1489387103826643030(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1489387103826643030(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1489387103826643030(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1489387103826643030);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14102609098454608700 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14102609098454608700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14102609098454608700(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14102609098454608700(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14102609098454608700);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13526847498537913373 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13526847498537913373(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13526847498537913373(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13526847498537913373(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13526847498537913373);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__271842712573538417 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__271842712573538417(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__271842712573538417(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__271842712573538417(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__271842712573538417);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9019456741387599675 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9019456741387599675(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9019456741387599675(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9019456741387599675(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9019456741387599675);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17283486799632228091 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17283486799632228091(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17283486799632228091(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17283486799632228091(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17283486799632228091);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13024976239489047069 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13024976239489047069(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13024976239489047069(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13024976239489047069(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13024976239489047069);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17297754191620100921 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17297754191620100921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17297754191620100921(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17297754191620100921(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17297754191620100921);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13260777379174997587 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13260777379174997587(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13260777379174997587(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13260777379174997587(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13260777379174997587);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1016761147811507029 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1016761147811507029(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1016761147811507029(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1016761147811507029(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1016761147811507029);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14808241629933017568 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14808241629933017568(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14808241629933017568(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14808241629933017568(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14808241629933017568);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10140154778741922857 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10140154778741922857(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10140154778741922857(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10140154778741922857(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10140154778741922857);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6858213468645653557 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6858213468645653557(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6858213468645653557(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6858213468645653557(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6858213468645653557);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18136048654831387763 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18136048654831387763(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18136048654831387763(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18136048654831387763(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18136048654831387763);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14902180229190095622 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14902180229190095622(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14902180229190095622(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14902180229190095622(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14902180229190095622);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17724587742610759636 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17724587742610759636(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17724587742610759636(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17724587742610759636(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17724587742610759636);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2634843001916455760 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2634843001916455760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2634843001916455760(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2634843001916455760(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2634843001916455760);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16686254346363701528 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      2048 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16686254346363701528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16686254346363701528(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16686254346363701528(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16686254346363701528);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13401139404442183813 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13401139404442183813(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13401139404442183813(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13401139404442183813(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13401139404442183813);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1332146888766722325 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1332146888766722325(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1332146888766722325(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1332146888766722325(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1332146888766722325);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__16541958848452560223 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16541958848452560223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16541958848452560223(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16541958848452560223(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16541958848452560223);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13887160502287526776 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13887160502287526776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13887160502287526776(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13887160502287526776(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13887160502287526776);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16946214994773352785 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16946214994773352785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16946214994773352785(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16946214994773352785(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16946214994773352785);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15101080895076611725 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15101080895076611725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15101080895076611725(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15101080895076611725(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15101080895076611725);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10982587854982486589 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10982587854982486589(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10982587854982486589(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10982587854982486589(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10982587854982486589);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__374629607233478748 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__374629607233478748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__374629607233478748(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__374629607233478748(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__374629607233478748);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16026079610504483964 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16026079610504483964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16026079610504483964(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16026079610504483964(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16026079610504483964);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12164964488286598512 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12164964488286598512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12164964488286598512(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12164964488286598512(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12164964488286598512);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10147244835092172528 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10147244835092172528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10147244835092172528(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10147244835092172528(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10147244835092172528);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17435117546757444542 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17435117546757444542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17435117546757444542(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17435117546757444542(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17435117546757444542);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7116379334535898366 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7116379334535898366(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7116379334535898366(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7116379334535898366(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7116379334535898366);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15637675421141326256 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15637675421141326256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15637675421141326256(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15637675421141326256(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15637675421141326256);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4707288886105408668 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      72 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4707288886105408668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4707288886105408668(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4707288886105408668(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4707288886105408668);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18201015683806244884 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18201015683806244884(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18201015683806244884(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18201015683806244884(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18201015683806244884);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8603707043522593448 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8603707043522593448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8603707043522593448(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8603707043522593448(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8603707043522593448);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12062490147426875651 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12062490147426875651(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12062490147426875651(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12062490147426875651(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12062490147426875651);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13664687729386736100 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13664687729386736100(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13664687729386736100(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13664687729386736100(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13664687729386736100);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4007814164972151618 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4007814164972151618(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4007814164972151618(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4007814164972151618(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4007814164972151618);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9686309343260265868 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9686309343260265868(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9686309343260265868(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9686309343260265868(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9686309343260265868);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14522820047005716411 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14522820047005716411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14522820047005716411(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14522820047005716411(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14522820047005716411);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8967089121756717985 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8967089121756717985(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8967089121756717985(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8967089121756717985(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8967089121756717985);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17369872665921109124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369872665921109124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17369872665921109124(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17369872665921109124(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17369872665921109124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__11645809780650099244 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11645809780650099244(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__11645809780650099244(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11645809780650099244(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__11645809780650099244);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__567192033102446298 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__567192033102446298(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__567192033102446298(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__567192033102446298(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__567192033102446298);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6765260634272608552 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6765260634272608552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6765260634272608552(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6765260634272608552(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6765260634272608552);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10958868604612717567 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10958868604612717567(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10958868604612717567(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10958868604612717567(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10958868604612717567);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11867751355202961772 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11867751355202961772(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11867751355202961772(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11867751355202961772(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11867751355202961772);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16691051966083552248 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16691051966083552248(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16691051966083552248(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16691051966083552248(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16691051966083552248);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7056083486109095584 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7056083486109095584(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7056083486109095584(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7056083486109095584(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7056083486109095584);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3782320729903775161 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3782320729903775161(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3782320729903775161(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3782320729903775161(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3782320729903775161);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5393071180449495672 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5393071180449495672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5393071180449495672(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5393071180449495672(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5393071180449495672);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1987009482875353349 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1987009482875353349(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1987009482875353349(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1987009482875353349(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1987009482875353349);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16834567321153154907 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16834567321153154907(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16834567321153154907(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16834567321153154907(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16834567321153154907);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3641414549369549848 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3641414549369549848(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3641414549369549848(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3641414549369549848(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3641414549369549848);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11012115219223475637 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11012115219223475637(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11012115219223475637(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11012115219223475637(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11012115219223475637);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7120249524013300472 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7120249524013300472(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7120249524013300472(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7120249524013300472(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7120249524013300472);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4432593669340601346 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4432593669340601346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4432593669340601346(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4432593669340601346(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4432593669340601346);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__844425238950818337 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__844425238950818337(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__844425238950818337(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__844425238950818337(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__844425238950818337);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7167074655531163951 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7167074655531163951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7167074655531163951(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7167074655531163951(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7167074655531163951);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__571093593728345146 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__571093593728345146(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__571093593728345146(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__571093593728345146(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__571093593728345146);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6058661173813504617 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6058661173813504617(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6058661173813504617(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6058661173813504617(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6058661173813504617);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14660330950287313950 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14660330950287313950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14660330950287313950(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14660330950287313950(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14660330950287313950);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6303500923586716196 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6303500923586716196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6303500923586716196(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6303500923586716196(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6303500923586716196);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6239278655394323750 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6239278655394323750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6239278655394323750(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6239278655394323750(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6239278655394323750);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__425723898112124084 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__425723898112124084(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__425723898112124084(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__425723898112124084(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__425723898112124084);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3718700080459184517 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3718700080459184517(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3718700080459184517(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3718700080459184517(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3718700080459184517);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3446710903852916443 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3446710903852916443(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3446710903852916443(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3446710903852916443(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3446710903852916443);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11830686672605457280 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11830686672605457280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11830686672605457280(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11830686672605457280(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11830686672605457280);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8948982159752244666 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8948982159752244666(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8948982159752244666(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8948982159752244666(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8948982159752244666);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17149033210160918388 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17149033210160918388(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17149033210160918388(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17149033210160918388(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17149033210160918388);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14006322033305905334 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14006322033305905334(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14006322033305905334(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14006322033305905334(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14006322033305905334);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__822973851033915131 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__822973851033915131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__822973851033915131(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__822973851033915131(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__822973851033915131);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12690241748540011838 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12690241748540011838(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12690241748540011838(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12690241748540011838(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12690241748540011838);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__4365182551978173004 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__4365182551978173004(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__4365182551978173004(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__4365182551978173004(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__4365182551978173004)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14049673442791329760 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14049673442791329760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14049673442791329760(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14049673442791329760(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14049673442791329760);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15652210265264554832 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2048 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15652210265264554832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15652210265264554832(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15652210265264554832(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15652210265264554832);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7076539361328403401 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7076539361328403401(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7076539361328403401(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7076539361328403401(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7076539361328403401);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14510518890677008067 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14510518890677008067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14510518890677008067(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14510518890677008067(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14510518890677008067);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12519890754418342413 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12519890754418342413(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12519890754418342413(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12519890754418342413(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12519890754418342413);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8114613659208523152 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8114613659208523152(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8114613659208523152(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8114613659208523152(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8114613659208523152);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18111679408650540675 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18111679408650540675(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18111679408650540675(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18111679408650540675(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18111679408650540675);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4968325115062548896 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4968325115062548896(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4968325115062548896(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4968325115062548896(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4968325115062548896);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8285343382329586140 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8285343382329586140(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8285343382329586140(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8285343382329586140(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8285343382329586140);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10440212990569573542 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10440212990569573542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10440212990569573542(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10440212990569573542(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10440212990569573542);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13069493441856767278 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13069493441856767278(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13069493441856767278(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13069493441856767278(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13069493441856767278);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__468173885789004228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__468173885789004228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__468173885789004228(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__468173885789004228(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__468173885789004228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3416959979253794129 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3416959979253794129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3416959979253794129(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3416959979253794129(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3416959979253794129);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16861998928816714988 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16861998928816714988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16861998928816714988(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16861998928816714988(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16861998928816714988);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13672476746408152044 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13672476746408152044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13672476746408152044(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13672476746408152044(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13672476746408152044);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__317784913690639558 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__317784913690639558(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__317784913690639558(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__317784913690639558(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__317784913690639558);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2779657889952314396 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2779657889952314396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2779657889952314396(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2779657889952314396(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2779657889952314396);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5566153394196348484 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5566153394196348484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5566153394196348484(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5566153394196348484(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5566153394196348484);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6208222326367766188 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6208222326367766188(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6208222326367766188(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6208222326367766188(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6208222326367766188);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4046394606799816150 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4046394606799816150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4046394606799816150(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4046394606799816150(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4046394606799816150);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15352464922327322769 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15352464922327322769(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15352464922327322769(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15352464922327322769(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15352464922327322769);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1275430822643153940 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1275430822643153940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1275430822643153940(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1275430822643153940(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1275430822643153940);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8921253312805449342 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8921253312805449342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8921253312805449342(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8921253312805449342(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8921253312805449342);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1214631080907759818 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1214631080907759818(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1214631080907759818(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1214631080907759818(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1214631080907759818);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17811105708275402227 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17811105708275402227(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17811105708275402227(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17811105708275402227(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17811105708275402227);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1043834622560449975 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1043834622560449975(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1043834622560449975(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1043834622560449975(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1043834622560449975);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14672848862245581221 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14672848862245581221(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14672848862245581221(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14672848862245581221(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14672848862245581221);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14709698274466547411 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14709698274466547411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14709698274466547411(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14709698274466547411(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14709698274466547411);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14373259734636879092 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14373259734636879092(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14373259734636879092(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14373259734636879092(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14373259734636879092);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3140975345526464392 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3140975345526464392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3140975345526464392(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3140975345526464392(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3140975345526464392);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14606149943093697544 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      6 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14606149943093697544(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 6} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14606149943093697544(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14606149943093697544(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14606149943093697544);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__625517316997492363 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1088 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__625517316997492363(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1088} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__625517316997492363(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__625517316997492363(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__625517316997492363);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4600806190213588003 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4600806190213588003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4600806190213588003(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4600806190213588003(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4600806190213588003);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11499577159931658988 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11499577159931658988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11499577159931658988(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11499577159931658988(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11499577159931658988);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7365682548855527202 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7365682548855527202(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7365682548855527202(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7365682548855527202(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7365682548855527202);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6614948510855390169 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6614948510855390169(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6614948510855390169(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6614948510855390169(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6614948510855390169);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8568864837973650096 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8568864837973650096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8568864837973650096(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8568864837973650096(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8568864837973650096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1722681732376591483 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1722681732376591483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1722681732376591483(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1722681732376591483(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1722681732376591483);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7855516766851368148 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7855516766851368148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7855516766851368148(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7855516766851368148(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7855516766851368148);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__427304825674646251 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__427304825674646251(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__427304825674646251(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__427304825674646251(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__427304825674646251);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3241114950652990020 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3241114950652990020(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3241114950652990020(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3241114950652990020(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3241114950652990020);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10510335012838655686 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10510335012838655686(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10510335012838655686(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10510335012838655686(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10510335012838655686);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1005720535189313487 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      80 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1005720535189313487(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 80} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1005720535189313487(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1005720535189313487(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1005720535189313487);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__622705393727988730 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__622705393727988730(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__622705393727988730(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__622705393727988730(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__622705393727988730);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9990143273663936976 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9990143273663936976(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9990143273663936976(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9990143273663936976(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9990143273663936976);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12572935449609919208 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12572935449609919208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12572935449609919208(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12572935449609919208(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12572935449609919208);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13559918207003632124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13559918207003632124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13559918207003632124(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13559918207003632124(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13559918207003632124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8853972608051346702 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8853972608051346702(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8853972608051346702(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8853972608051346702(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8853972608051346702);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7194521522656121228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7194521522656121228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7194521522656121228(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7194521522656121228(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7194521522656121228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17662379094435032943 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17662379094435032943(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17662379094435032943(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17662379094435032943(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17662379094435032943);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17685984956642119489 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17685984956642119489(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17685984956642119489(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17685984956642119489(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17685984956642119489);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9128589725501070319 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9128589725501070319(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9128589725501070319(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9128589725501070319(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9128589725501070319);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17037438135110514962 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17037438135110514962(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17037438135110514962(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17037438135110514962(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17037438135110514962);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1730355091918677120 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1730355091918677120(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1730355091918677120(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1730355091918677120(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1730355091918677120);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16515451873221632118 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16515451873221632118(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16515451873221632118(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16515451873221632118(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16515451873221632118);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9920391819615103996 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1968 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9920391819615103996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1968} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9920391819615103996(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9920391819615103996(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9920391819615103996);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11996732445629369457 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11996732445629369457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11996732445629369457(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11996732445629369457(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11996732445629369457);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__458765587276959468 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__458765587276959468(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__458765587276959468(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__458765587276959468(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__458765587276959468);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1762934454737105752 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1376 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1762934454737105752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1376} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1762934454737105752(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1762934454737105752(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1762934454737105752);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17636107891280127200 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      816 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17636107891280127200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 816} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17636107891280127200(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17636107891280127200(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17636107891280127200);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6539161496449033381 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6539161496449033381(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6539161496449033381(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6539161496449033381(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6539161496449033381);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2008566541283334776 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2008566541283334776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2008566541283334776(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2008566541283334776(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2008566541283334776);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6070277950546989393 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1392 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6070277950546989393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1392} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6070277950546989393(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6070277950546989393(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6070277950546989393);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16224937666929154496 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16224937666929154496(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16224937666929154496(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16224937666929154496(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16224937666929154496);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2476793740556602268 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2476793740556602268(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2476793740556602268(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2476793740556602268(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2476793740556602268);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__18317491714608294765 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18317491714608294765(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__18317491714608294765(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18317491714608294765(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__18317491714608294765);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5479898369127167297 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1680 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5479898369127167297(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1680} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5479898369127167297(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5479898369127167297(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5479898369127167297);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2169043732354201046 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2169043732354201046(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2169043732354201046(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2169043732354201046(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2169043732354201046);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15553281524985187560 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15553281524985187560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15553281524985187560(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15553281524985187560(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15553281524985187560);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13486637249334895925 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13486637249334895925(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13486637249334895925(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13486637249334895925(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13486637249334895925);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12663404680355148997 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1696 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12663404680355148997(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1696} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12663404680355148997(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12663404680355148997(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12663404680355148997);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13970260044162223236 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13970260044162223236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13970260044162223236(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13970260044162223236(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13970260044162223236);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8983284938202998601 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8983284938202998601(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8983284938202998601(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8983284938202998601(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8983284938202998601);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4393573331674098613 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4393573331674098613(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4393573331674098613(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4393573331674098613(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4393573331674098613);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4125851323263220278 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      912 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4125851323263220278(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 912} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4125851323263220278(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4125851323263220278(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4125851323263220278);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5226262561008816636 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5226262561008816636(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5226262561008816636(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5226262561008816636(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5226262561008816636);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3302089724240251085 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3302089724240251085(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3302089724240251085(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3302089724240251085(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3302089724240251085);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4755348409543008311 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1680 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4755348409543008311(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1680} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4755348409543008311(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4755348409543008311(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4755348409543008311);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5432275922898918675 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5432275922898918675(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5432275922898918675(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5432275922898918675(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5432275922898918675);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12131967263064106304 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12131967263064106304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12131967263064106304(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12131967263064106304(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12131967263064106304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15239050269727217225 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1312 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15239050269727217225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1312} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15239050269727217225(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15239050269727217225(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15239050269727217225);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13224460850867317105 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13224460850867317105(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13224460850867317105(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13224460850867317105(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13224460850867317105);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__907418252938748303 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1584 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__907418252938748303(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1584} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__907418252938748303(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__907418252938748303(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__907418252938748303);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6674206196115590345 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      120 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6674206196115590345(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 120} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6674206196115590345(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6674206196115590345(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6674206196115590345);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11949244982357724085 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11949244982357724085(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11949244982357724085(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11949244982357724085(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11949244982357724085);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6994795948242090707 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6994795948242090707(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6994795948242090707(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6994795948242090707(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6994795948242090707);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__9221103811745696468 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9221103811745696468(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__9221103811745696468(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9221103811745696468(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__9221103811745696468);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8209282571089440118 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1088 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8209282571089440118(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1088} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8209282571089440118(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8209282571089440118(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8209282571089440118);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16817911801268191622 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16817911801268191622(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16817911801268191622(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16817911801268191622(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16817911801268191622);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10033041076558085189 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      80 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10033041076558085189(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 80} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10033041076558085189(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10033041076558085189(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10033041076558085189);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11822174189876461638 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1200 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11822174189876461638(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1200} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11822174189876461638(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11822174189876461638(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11822174189876461638);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14138614616455762898 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14138614616455762898(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14138614616455762898(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14138614616455762898(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14138614616455762898);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6368562198635526368 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6368562198635526368(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6368562198635526368(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6368562198635526368(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6368562198635526368);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8888071836016094927 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8888071836016094927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8888071836016094927(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8888071836016094927(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8888071836016094927);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16172859449892645424 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      240 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16172859449892645424(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 240} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16172859449892645424(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16172859449892645424(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16172859449892645424);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13433531517917758342 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13433531517917758342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13433531517917758342(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13433531517917758342(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13433531517917758342);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8074332121187129346 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8074332121187129346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8074332121187129346(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8074332121187129346(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8074332121187129346);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13457935975816752255 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      109 /* Input2 */, \
      109 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13457935975816752255(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 109} /* Input2 */, 
      {"input[3]", 109} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13457935975816752255(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13457935975816752255(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13457935975816752255);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9117152362512967211 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9117152362512967211(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9117152362512967211(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9117152362512967211(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9117152362512967211);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__935598699366964600 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__935598699366964600(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__935598699366964600(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__935598699366964600(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__935598699366964600);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2829936442301039216 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1184 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2829936442301039216(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1184} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2829936442301039216(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2829936442301039216(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2829936442301039216);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14631896582667367105 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      624 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14631896582667367105(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 624} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14631896582667367105(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14631896582667367105(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14631896582667367105);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1660865448488862633 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1660865448488862633(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1660865448488862633(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1660865448488862633(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1660865448488862633);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1752448317128014126 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1752448317128014126(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1752448317128014126(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1752448317128014126(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1752448317128014126);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8763255964967586191 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8763255964967586191(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8763255964967586191(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8763255964967586191(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8763255964967586191);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9668267305734953976 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1584 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9668267305734953976(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1584} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9668267305734953976(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9668267305734953976(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9668267305734953976);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2633433856183067547 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2633433856183067547(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2633433856183067547(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2633433856183067547(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2633433856183067547);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2329942898477656564 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2329942898477656564(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2329942898477656564(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2329942898477656564(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2329942898477656564);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3246264060818295067 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2016 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3246264060818295067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2016} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3246264060818295067(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3246264060818295067(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3246264060818295067);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12916687554404156491 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1088 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12916687554404156491(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1088} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12916687554404156491(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12916687554404156491(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12916687554404156491);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13736083495034629107 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13736083495034629107(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13736083495034629107(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13736083495034629107(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13736083495034629107);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17890393219155952027 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17890393219155952027(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17890393219155952027(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17890393219155952027(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17890393219155952027);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16023927660845710905 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16023927660845710905(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16023927660845710905(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16023927660845710905(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16023927660845710905);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5815036333269069931 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1792 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5815036333269069931(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1792} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5815036333269069931(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5815036333269069931(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5815036333269069931);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5952181353183619546 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5952181353183619546(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5952181353183619546(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5952181353183619546(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5952181353183619546);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__820835021015625390 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__820835021015625390(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__820835021015625390(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__820835021015625390(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__820835021015625390);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15188191995783114700 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15188191995783114700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15188191995783114700(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15188191995783114700(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15188191995783114700);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5646201255821038064 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1968 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5646201255821038064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1968} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5646201255821038064(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5646201255821038064(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5646201255821038064);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14253545404983658136 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14253545404983658136(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14253545404983658136(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14253545404983658136(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14253545404983658136);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17971678796398909071 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      336 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17971678796398909071(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 336} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17971678796398909071(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17971678796398909071(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17971678796398909071);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11168815270011969912 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11168815270011969912(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11168815270011969912(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11168815270011969912(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11168815270011969912);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1870965837309866772 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1568 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1870965837309866772(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1568} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1870965837309866772(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1870965837309866772(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1870965837309866772);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8537625993366978115 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537625993366978115(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8537625993366978115(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8537625993366978115(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8537625993366978115);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12279064804691543690 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12279064804691543690(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12279064804691543690(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12279064804691543690(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12279064804691543690);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1343437070901262044 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1343437070901262044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1343437070901262044(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1343437070901262044(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1343437070901262044);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12455095669687506839 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12455095669687506839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12455095669687506839(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12455095669687506839(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12455095669687506839);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10247714117544112876 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10247714117544112876(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10247714117544112876(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10247714117544112876(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10247714117544112876);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__18053561199725640749 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1680 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18053561199725640749(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1680} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18053561199725640749(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18053561199725640749(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18053561199725640749);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5475462865468982082 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1120 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5475462865468982082(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1120} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5475462865468982082(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5475462865468982082(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5475462865468982082);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12538589766260334477 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12538589766260334477(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12538589766260334477(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12538589766260334477(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12538589766260334477);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10497063570328578752 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1968 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10497063570328578752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1968} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10497063570328578752(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10497063570328578752(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10497063570328578752);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5734798971605670099 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5734798971605670099(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5734798971605670099(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5734798971605670099(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5734798971605670099);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14621612245759666345 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14621612245759666345(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14621612245759666345(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14621612245759666345(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14621612245759666345);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10925947805390618209 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10925947805390618209(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10925947805390618209(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10925947805390618209(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10925947805390618209);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13273095213717627052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13273095213717627052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13273095213717627052(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13273095213717627052(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13273095213717627052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1170863943628165605 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1170863943628165605(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1170863943628165605(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1170863943628165605(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1170863943628165605);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10960804982437721652 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      528 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10960804982437721652(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10960804982437721652(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10960804982437721652(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10960804982437721652);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12438419869829282282 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12438419869829282282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12438419869829282282(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12438419869829282282(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12438419869829282282);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4747951221880117499 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4747951221880117499(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4747951221880117499(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4747951221880117499(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4747951221880117499);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12469847824775853459 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1472 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12469847824775853459(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1472} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12469847824775853459(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12469847824775853459(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12469847824775853459);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5005970178315745175 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5005970178315745175(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5005970178315745175(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5005970178315745175(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5005970178315745175);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9353995607805530229 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9353995607805530229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9353995607805530229(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9353995607805530229(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9353995607805530229);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17065329912294635005 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17065329912294635005(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17065329912294635005(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17065329912294635005(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17065329912294635005);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__258474056135478596 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__258474056135478596(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__258474056135478596(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__258474056135478596(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__258474056135478596);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10644980691349767752 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1008 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10644980691349767752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1008} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10644980691349767752(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10644980691349767752(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10644980691349767752);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8153782823842674263 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8153782823842674263(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8153782823842674263(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8153782823842674263(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8153782823842674263);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17681494405780982153 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17681494405780982153(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17681494405780982153(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17681494405780982153(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17681494405780982153);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2197249491511363692 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2197249491511363692(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2197249491511363692(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2197249491511363692(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2197249491511363692);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12848070190982312252 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      624 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12848070190982312252(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 624} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12848070190982312252(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12848070190982312252(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12848070190982312252);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6677743349119782522 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6677743349119782522(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6677743349119782522(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6677743349119782522(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6677743349119782522);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14760720454260450409 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14760720454260450409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14760720454260450409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14760720454260450409(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14760720454260450409);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1566496286798544400 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1566496286798544400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1566496286798544400(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1566496286798544400(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1566496286798544400);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7808721013870303425 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1104 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7808721013870303425(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1104} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7808721013870303425(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7808721013870303425(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7808721013870303425);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15895257724689565768 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15895257724689565768(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15895257724689565768(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15895257724689565768(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15895257724689565768);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14928471004910507302 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1776 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14928471004910507302(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1776} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14928471004910507302(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14928471004910507302(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14928471004910507302);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3956942270523253434 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1856 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3956942270523253434(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1856} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3956942270523253434(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3956942270523253434(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3956942270523253434);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3028457482681800476 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3028457482681800476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3028457482681800476(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3028457482681800476(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3028457482681800476);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__823806921665878066 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__823806921665878066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__823806921665878066(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__823806921665878066(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__823806921665878066);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13751823895188384672 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13751823895188384672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13751823895188384672(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13751823895188384672(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13751823895188384672);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__16654370980763937462 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__16654370980763937462(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__16654370980763937462(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__16654370980763937462(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__16654370980763937462)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3560366845815364133 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1296 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3560366845815364133(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1296} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3560366845815364133(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3560366845815364133(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3560366845815364133);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14540772267762788994 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14540772267762788994(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14540772267762788994(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14540772267762788994(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14540772267762788994);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16603075659333654684 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16603075659333654684(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16603075659333654684(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16603075659333654684(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16603075659333654684);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10828661803945181479 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      108 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10828661803945181479(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 108} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10828661803945181479(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10828661803945181479(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10828661803945181479);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3188059485065342570 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3188059485065342570(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3188059485065342570(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3188059485065342570(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3188059485065342570);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16923706003881364027 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1120 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16923706003881364027(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1120} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16923706003881364027(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16923706003881364027(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16923706003881364027);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2826468601497134136 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2826468601497134136(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2826468601497134136(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2826468601497134136(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2826468601497134136);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13190090960542034621 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13190090960542034621(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13190090960542034621(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13190090960542034621(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13190090960542034621);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12890179422141723224 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12890179422141723224(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12890179422141723224(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12890179422141723224(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12890179422141723224);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6849223087491717565 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6849223087491717565(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6849223087491717565(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6849223087491717565(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6849223087491717565);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12554245897602829672 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12554245897602829672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12554245897602829672(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12554245897602829672(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12554245897602829672);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5120480249333354387 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1872 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5120480249333354387(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1872} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5120480249333354387(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5120480249333354387(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5120480249333354387);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3805367990624450528 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2208 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3805367990624450528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2208} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3805367990624450528(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3805367990624450528(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3805367990624450528);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16651877053775364993 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16651877053775364993(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16651877053775364993(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16651877053775364993(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16651877053775364993);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14482815302186012978 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14482815302186012978(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14482815302186012978(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14482815302186012978(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14482815302186012978);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14743054462510539326 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14743054462510539326(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14743054462510539326(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14743054462510539326(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14743054462510539326);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16094851221978355591 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1696 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16094851221978355591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1696} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16094851221978355591(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16094851221978355591(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16094851221978355591);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15470162732405892058 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15470162732405892058(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15470162732405892058(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15470162732405892058(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15470162732405892058);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1147396331563806386 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1488 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1147396331563806386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1488} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1147396331563806386(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1147396331563806386(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1147396331563806386);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2843073533117737814 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1120 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2843073533117737814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1120} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2843073533117737814(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2843073533117737814(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2843073533117737814);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11779285660489579751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1056 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11779285660489579751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1056} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11779285660489579751(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11779285660489579751(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11779285660489579751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17005215014494021622 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17005215014494021622(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17005215014494021622(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17005215014494021622(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17005215014494021622);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9689935396382277026 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9689935396382277026(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9689935396382277026(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9689935396382277026(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9689935396382277026);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5548370639619174779 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5548370639619174779(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5548370639619174779(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5548370639619174779(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5548370639619174779);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7927306540370792213 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7927306540370792213(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7927306540370792213(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7927306540370792213(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7927306540370792213);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18109252688861708704 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18109252688861708704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18109252688861708704(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18109252688861708704(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18109252688861708704);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10041581801526322250 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2208 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      7 /* StrideHeight */, \
      7 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10041581801526322250(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2208} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 7} /* StrideHeight */, 
      {"stride_width", 7} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10041581801526322250(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10041581801526322250(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10041581801526322250);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10176128455826911528 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10176128455826911528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10176128455826911528(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10176128455826911528(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10176128455826911528);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5854161507778347640 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5854161507778347640(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5854161507778347640(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5854161507778347640(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5854161507778347640);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10006728662205644810 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10006728662205644810(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10006728662205644810(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10006728662205644810(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10006728662205644810);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__175279574158201503 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__175279574158201503(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__175279574158201503(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__175279574158201503(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__175279574158201503);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5494256108005166104 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5494256108005166104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5494256108005166104(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5494256108005166104(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5494256108005166104);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4918949358912530449 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4918949358912530449(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4918949358912530449(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4918949358912530449(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4918949358912530449);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15394165101081299013 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15394165101081299013(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15394165101081299013(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15394165101081299013(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15394165101081299013);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4010013081835961537 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      528 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4010013081835961537(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4010013081835961537(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4010013081835961537(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4010013081835961537);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15664602370867557788 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15664602370867557788(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15664602370867557788(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15664602370867557788(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15664602370867557788);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10509062094803228287 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10509062094803228287(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10509062094803228287(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10509062094803228287(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10509062094803228287);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__645605504867960240 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__645605504867960240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__645605504867960240(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__645605504867960240(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__645605504867960240);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2372262203518232688 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      149 /* Input2 */, \
      149 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2372262203518232688(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 149} /* Input2 */, 
      {"input[3]", 149} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2372262203518232688(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2372262203518232688(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2372262203518232688);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__8609296745500688544 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8609296745500688544(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8609296745500688544(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8609296745500688544(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8609296745500688544);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12713982363157471096 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12713982363157471096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12713982363157471096(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12713982363157471096(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12713982363157471096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15547794162757586687 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15547794162757586687(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15547794162757586687(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15547794162757586687(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15547794162757586687);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7974540441954992006 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7974540441954992006(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7974540441954992006(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7974540441954992006(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7974540441954992006);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2166999985863903480 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2166999985863903480(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2166999985863903480(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2166999985863903480(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2166999985863903480);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12775988784791043972 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12775988784791043972(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12775988784791043972(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12775988784791043972(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12775988784791043972);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7569279784605859773 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      624 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7569279784605859773(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 624} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7569279784605859773(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7569279784605859773(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7569279784605859773);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15009239619306509629 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1216 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15009239619306509629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1216} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15009239619306509629(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15009239619306509629(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15009239619306509629);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9490359356203946525 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9490359356203946525(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9490359356203946525(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9490359356203946525(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9490359356203946525);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13276501164795467960 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13276501164795467960(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13276501164795467960(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13276501164795467960(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13276501164795467960);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__9551531291328422216 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      7 /* StrideHeight */, \
      7 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9551531291328422216(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 7} /* StrideHeight */, 
      {"stride_width", 7} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__9551531291328422216(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9551531291328422216(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__9551531291328422216);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16123700676389766232 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16123700676389766232(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16123700676389766232(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16123700676389766232(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16123700676389766232);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14293289303414316309 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1488 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14293289303414316309(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1488} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14293289303414316309(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14293289303414316309(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14293289303414316309);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17298430291974151657 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17298430291974151657(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17298430291974151657(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17298430291974151657(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17298430291974151657);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13267629027663697949 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13267629027663697949(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13267629027663697949(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13267629027663697949(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13267629027663697949);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18274326794381023857 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18274326794381023857(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18274326794381023857(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18274326794381023857(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18274326794381023857);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7743177773819898636 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7743177773819898636(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7743177773819898636(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7743177773819898636(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7743177773819898636);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16215455595259722436 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16215455595259722436(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16215455595259722436(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16215455595259722436(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16215455595259722436);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10530935102687869983 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1760 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10530935102687869983(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1760} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10530935102687869983(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10530935102687869983(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10530935102687869983);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9968068155292019821 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      120 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9968068155292019821(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 120} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9968068155292019821(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9968068155292019821(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9968068155292019821);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7314538649809509497 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7314538649809509497(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7314538649809509497(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7314538649809509497(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7314538649809509497);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9067302736487515626 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1408 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9067302736487515626(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1408} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9067302736487515626(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9067302736487515626(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9067302736487515626);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10691297493486163267 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10691297493486163267(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10691297493486163267(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10691297493486163267(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10691297493486163267);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2554534601468680795 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      36 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      8 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2554534601468680795(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 36} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2554534601468680795(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2554534601468680795(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2554534601468680795);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4489220245340408942 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1872 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4489220245340408942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1872} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4489220245340408942(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4489220245340408942(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4489220245340408942);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9168518837117353416 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9168518837117353416(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9168518837117353416(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9168518837117353416(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9168518837117353416);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9917310424893953524 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      72 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9917310424893953524(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9917310424893953524(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9917310424893953524(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9917310424893953524);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13148845154237207365 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1312 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13148845154237207365(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1312} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13148845154237207365(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13148845154237207365(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13148845154237207365);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14671088864801548462 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1792 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14671088864801548462(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1792} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14671088864801548462(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14671088864801548462(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14671088864801548462);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13650864994840300686 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13650864994840300686(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13650864994840300686(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13650864994840300686(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13650864994840300686);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14850848177056927794 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14850848177056927794(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14850848177056927794(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14850848177056927794(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14850848177056927794);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5202622821518772751 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5202622821518772751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5202622821518772751(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5202622821518772751(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5202622821518772751);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4555512942152606277 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4555512942152606277(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4555512942152606277(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4555512942152606277(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4555512942152606277);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9478810876736711015 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9478810876736711015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9478810876736711015(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9478810876736711015(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9478810876736711015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__151914647175195954 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__151914647175195954(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__151914647175195954(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__151914647175195954(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__151914647175195954);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5740560829027115335 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1856 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5740560829027115335(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1856} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5740560829027115335(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5740560829027115335(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5740560829027115335);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12315920806723583712 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      8 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12315920806723583712(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12315920806723583712(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12315920806723583712(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12315920806723583712);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9933834057497793338 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9933834057497793338(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9933834057497793338(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9933834057497793338(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9933834057497793338);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14064194159521710203 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14064194159521710203(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14064194159521710203(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14064194159521710203(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14064194159521710203);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12363290518244005445 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1776 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12363290518244005445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1776} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12363290518244005445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12363290518244005445(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12363290518244005445);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3852614413094881799 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3852614413094881799(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3852614413094881799(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3852614413094881799(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3852614413094881799);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2490058696250499108 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2490058696250499108(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2490058696250499108(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2490058696250499108(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2490058696250499108);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3713442703168870673 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3713442703168870673(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3713442703168870673(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3713442703168870673(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3713442703168870673);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11225946857983073150 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11225946857983073150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11225946857983073150(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11225946857983073150(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11225946857983073150);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12059872547139522454 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12059872547139522454(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12059872547139522454(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12059872547139522454(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12059872547139522454);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12522537475654558722 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12522537475654558722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12522537475654558722(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12522537475654558722(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12522537475654558722);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13201157111984486303 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13201157111984486303(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13201157111984486303(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13201157111984486303(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13201157111984486303);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10387070966862316155 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10387070966862316155(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10387070966862316155(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10387070966862316155(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10387070966862316155);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10734144161847870641 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10734144161847870641(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10734144161847870641(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10734144161847870641(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10734144161847870641);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7658529096259321857 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7658529096259321857(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7658529096259321857(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7658529096259321857(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7658529096259321857);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15254329239818398004 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15254329239818398004(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15254329239818398004(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15254329239818398004(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15254329239818398004);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4348806035540610988 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4348806035540610988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4348806035540610988(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4348806035540610988(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4348806035540610988);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12586428751697793206 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12586428751697793206(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12586428751697793206(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12586428751697793206(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12586428751697793206);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__945051350873084581 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__945051350873084581(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__945051350873084581(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__945051350873084581(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__945051350873084581);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18036004818766463700 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1888 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18036004818766463700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1888} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18036004818766463700(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18036004818766463700(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18036004818766463700);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6733583289764729139 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1872 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6733583289764729139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1872} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6733583289764729139(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6733583289764729139(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6733583289764729139);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__18206738443545283350 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1000 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      13 /* FilterHeight */, \
      13 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      13 /* StrideHeight */, \
      13 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18206738443545283350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_height", 13} /* FilterHeight */, 
      {"filter_width", 13} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 13} /* StrideHeight */, 
      {"stride_width", 13} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__18206738443545283350(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18206738443545283350(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__18206738443545283350);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7102193652558196829 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7102193652558196829(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7102193652558196829(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7102193652558196829(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7102193652558196829);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2945583165946513987 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2945583165946513987(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2945583165946513987(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2945583165946513987(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2945583165946513987);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4169885408281799562 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      147 /* Input2 */, \
      147 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4169885408281799562(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 147} /* Input2 */, 
      {"input[3]", 147} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4169885408281799562(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4169885408281799562(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4169885408281799562);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__18119236745019945700 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18119236745019945700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18119236745019945700(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18119236745019945700(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18119236745019945700);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11779731799998450113 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1104 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11779731799998450113(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1104} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11779731799998450113(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11779731799998450113(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11779731799998450113);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17072285397383335860 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17072285397383335860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17072285397383335860(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17072285397383335860(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17072285397383335860);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1251076874104384230 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1251076874104384230(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1251076874104384230(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1251076874104384230(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1251076874104384230);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9071958956850115501 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9071958956850115501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9071958956850115501(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9071958956850115501(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9071958956850115501);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4351901593925929661 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4351901593925929661(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4351901593925929661(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4351901593925929661(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4351901593925929661);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10432925476974570992 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      624 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10432925476974570992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 624} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10432925476974570992(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10432925476974570992(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10432925476974570992);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7775371618473473313 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7775371618473473313(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7775371618473473313(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7775371618473473313(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7775371618473473313);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10157964876296523727 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      448 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10157964876296523727(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 448} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10157964876296523727(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10157964876296523727(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10157964876296523727);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16365413992503192949 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1696 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16365413992503192949(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1696} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16365413992503192949(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16365413992503192949(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16365413992503192949);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15298375163550889582 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1472 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15298375163550889582(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1472} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15298375163550889582(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15298375163550889582(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15298375163550889582);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4327654060257206067 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4327654060257206067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4327654060257206067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4327654060257206067(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4327654060257206067);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13803404248510772083 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2016 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13803404248510772083(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2016} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13803404248510772083(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13803404248510772083(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13803404248510772083);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3178325156232993819 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3178325156232993819(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3178325156232993819(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3178325156232993819(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3178325156232993819);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5399800325006550321 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5399800325006550321(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5399800325006550321(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5399800325006550321(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5399800325006550321);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__869176063466144504 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__869176063466144504(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__869176063466144504(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__869176063466144504(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__869176063466144504);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17575782438175700551 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17575782438175700551(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17575782438175700551(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17575782438175700551(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17575782438175700551);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15487260949946451599 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15487260949946451599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15487260949946451599(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15487260949946451599(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15487260949946451599);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3514306177153898736 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3514306177153898736(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3514306177153898736(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3514306177153898736(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3514306177153898736);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11538410759652882017 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11538410759652882017(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11538410759652882017(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11538410759652882017(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11538410759652882017);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11014730391047227625 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1776 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11014730391047227625(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1776} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11014730391047227625(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11014730391047227625(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11014730391047227625);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11925603990265158485 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11925603990265158485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11925603990265158485(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11925603990265158485(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11925603990265158485);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16426158785412765292 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1488 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16426158785412765292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1488} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16426158785412765292(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16426158785412765292(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16426158785412765292);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15190721354149746104 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1776 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15190721354149746104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1776} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15190721354149746104(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15190721354149746104(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15190721354149746104);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13226307354641549290 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13226307354641549290(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13226307354641549290(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13226307354641549290(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13226307354641549290);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4723523119508942273 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4723523119508942273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4723523119508942273(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4723523119508942273(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4723523119508942273);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15415139693877327075 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1296 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15415139693877327075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1296} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15415139693877327075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15415139693877327075(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15415139693877327075);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__260314335461162739 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__260314335461162739(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__260314335461162739(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__260314335461162739(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__260314335461162739);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9317281935353895047 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9317281935353895047(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9317281935353895047(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9317281935353895047(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9317281935353895047);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12906477199126717865 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12906477199126717865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12906477199126717865(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12906477199126717865(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12906477199126717865);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8997401451905081145 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8997401451905081145(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8997401451905081145(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8997401451905081145(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8997401451905081145);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16511680373526488165 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2064 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16511680373526488165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2064} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16511680373526488165(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16511680373526488165(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16511680373526488165);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9131512290794118481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9131512290794118481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9131512290794118481(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9131512290794118481(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9131512290794118481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5526123440148330063 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5526123440148330063(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5526123440148330063(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5526123440148330063(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5526123440148330063);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12744600835869511338 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12744600835869511338(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12744600835869511338(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12744600835869511338(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12744600835869511338);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12648551831957927026 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1760 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12648551831957927026(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1760} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12648551831957927026(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12648551831957927026(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12648551831957927026);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16542121897841281351 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1376 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16542121897841281351(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1376} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16542121897841281351(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16542121897841281351(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16542121897841281351);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9929687760008916436 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9929687760008916436(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9929687760008916436(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9929687760008916436(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9929687760008916436);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6835957008451454230 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6835957008451454230(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6835957008451454230(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6835957008451454230(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6835957008451454230);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__220407799448248782 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__220407799448248782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__220407799448248782(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__220407799448248782(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__220407799448248782);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__820959239065469817 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1216 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__820959239065469817(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1216} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__820959239065469817(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__820959239065469817(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__820959239065469817);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15830920874395953247 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      111 /* Input2 */, \
      111 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15830920874395953247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 111} /* Input2 */, 
      {"input[3]", 111} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15830920874395953247(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15830920874395953247(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15830920874395953247);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9198481491418160798 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9198481491418160798(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9198481491418160798(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9198481491418160798(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9198481491418160798);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8604594854201981062 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8604594854201981062(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8604594854201981062(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8604594854201981062(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8604594854201981062);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4093583425087660423 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1472 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4093583425087660423(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1472} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4093583425087660423(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4093583425087660423(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4093583425087660423);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15140794318905546968 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15140794318905546968(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15140794318905546968(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15140794318905546968(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15140794318905546968);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8851045959010275001 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8851045959010275001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8851045959010275001(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8851045959010275001(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8851045959010275001);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9332812659044847605 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9332812659044847605(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9332812659044847605(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9332812659044847605(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9332812659044847605);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6760928264120547501 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6760928264120547501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6760928264120547501(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6760928264120547501(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6760928264120547501);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8182210308700793161 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1312 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8182210308700793161(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1312} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8182210308700793161(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8182210308700793161(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8182210308700793161);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15948301148017525691 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1200 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15948301148017525691(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1200} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15948301148017525691(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15948301148017525691(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15948301148017525691);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9342917332437524685 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9342917332437524685(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9342917332437524685(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9342917332437524685(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9342917332437524685);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13568631438374962887 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13568631438374962887(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13568631438374962887(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13568631438374962887(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13568631438374962887);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2474735651427352560 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1968 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2474735651427352560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1968} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2474735651427352560(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2474735651427352560(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2474735651427352560);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2561496082363062072 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2561496082363062072(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2561496082363062072(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2561496082363062072(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2561496082363062072);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14627440503972881022 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14627440503972881022(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14627440503972881022(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14627440503972881022(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14627440503972881022);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12740446513617701147 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12740446513617701147(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12740446513617701147(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12740446513617701147(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12740446513617701147);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11800216629943839199 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11800216629943839199(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11800216629943839199(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11800216629943839199(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11800216629943839199);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13252641046609426856 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      80 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13252641046609426856(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 80} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13252641046609426856(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13252641046609426856(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13252641046609426856);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9235555771543721330 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      336 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9235555771543721330(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 336} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9235555771543721330(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9235555771543721330(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9235555771543721330);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5455464030539880251 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1568 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5455464030539880251(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1568} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5455464030539880251(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5455464030539880251(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5455464030539880251);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8193971207190333005 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8193971207190333005(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8193971207190333005(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8193971207190333005(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8193971207190333005);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15956937228742235558 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15956937228742235558(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15956937228742235558(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15956937228742235558(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15956937228742235558);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16784489324997126461 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1968 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16784489324997126461(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1968} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16784489324997126461(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16784489324997126461(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16784489324997126461);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2477391923258239139 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2477391923258239139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2477391923258239139(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2477391923258239139(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2477391923258239139);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__42312388863137085 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__42312388863137085(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__42312388863137085(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__42312388863137085(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__42312388863137085);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18372930219674482937 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18372930219674482937(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18372930219674482937(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18372930219674482937(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18372930219674482937);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3509377393695962725 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3509377393695962725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3509377393695962725(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3509377393695962725(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3509377393695962725);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16931865656217547543 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2016 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16931865656217547543(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2016} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16931865656217547543(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16931865656217547543(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16931865656217547543);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5168843015962210210 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5168843015962210210(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5168843015962210210(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5168843015962210210(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5168843015962210210);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7803115961948562434 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803115961948562434(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7803115961948562434(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7803115961948562434(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7803115961948562434);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15477707876501108623 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1760 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15477707876501108623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1760} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15477707876501108623(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15477707876501108623(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15477707876501108623);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6483939235011946178 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6483939235011946178(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6483939235011946178(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6483939235011946178(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6483939235011946178);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13663179459878813664 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13663179459878813664(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13663179459878813664(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13663179459878813664(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13663179459878813664);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16257815717376363284 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1776 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16257815717376363284(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1776} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16257815717376363284(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16257815717376363284(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16257815717376363284);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15701647760291234752 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15701647760291234752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15701647760291234752(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15701647760291234752(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15701647760291234752);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13362664055181433895 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13362664055181433895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13362664055181433895(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13362664055181433895(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13362664055181433895);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3611923755672461038 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3611923755672461038(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3611923755672461038(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3611923755672461038(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3611923755672461038);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12659264956869150066 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12659264956869150066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12659264956869150066(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12659264956869150066(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12659264956869150066);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__12820671032486229210 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12820671032486229210(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12820671032486229210(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12820671032486229210(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12820671032486229210)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15510249241594337557 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15510249241594337557(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15510249241594337557(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15510249241594337557(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15510249241594337557);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9287220948817869332 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      40 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      240 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9287220948817869332(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 40} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 240} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9287220948817869332(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9287220948817869332(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9287220948817869332);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8461174267083271571 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8461174267083271571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8461174267083271571(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8461174267083271571(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8461174267083271571);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15133949917012685436 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15133949917012685436(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15133949917012685436(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15133949917012685436(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15133949917012685436);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12371325326247347380 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      4096 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12371325326247347380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 4096} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12371325326247347380(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12371325326247347380(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12371325326247347380);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13097143762884142361 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13097143762884142361(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13097143762884142361(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13097143762884142361(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13097143762884142361);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__917557331131417638 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__917557331131417638(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__917557331131417638(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__917557331131417638(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__917557331131417638);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7548823366359819774 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7548823366359819774(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7548823366359819774(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7548823366359819774(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7548823366359819774);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11006021407263199273 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11006021407263199273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11006021407263199273(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11006021407263199273(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11006021407263199273);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14529476037974548170 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14529476037974548170(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14529476037974548170(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14529476037974548170(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14529476037974548170);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6705828126482246821 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1392 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6705828126482246821(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1392} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6705828126482246821(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6705828126482246821(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6705828126482246821);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6990962005955514041 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1216 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6990962005955514041(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1216} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6990962005955514041(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6990962005955514041(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6990962005955514041);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7745114859528059912 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7745114859528059912(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7745114859528059912(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7745114859528059912(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7745114859528059912);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5343378763786940039 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5343378763786940039(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5343378763786940039(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5343378763786940039(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5343378763786940039);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10097697997737101940 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10097697997737101940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10097697997737101940(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10097697997737101940(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10097697997737101940);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6455987289498881909 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6455987289498881909(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6455987289498881909(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6455987289498881909(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6455987289498881909);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__17861013762281255035 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17861013762281255035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17861013762281255035(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17861013762281255035(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17861013762281255035);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14315148003418179608 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14315148003418179608(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14315148003418179608(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14315148003418179608(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14315148003418179608);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1277751286658646454 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1277751286658646454(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1277751286658646454(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1277751286658646454(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1277751286658646454);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10479861929518278491 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10479861929518278491(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10479861929518278491(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10479861929518278491(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10479861929518278491);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4935520814232672116 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4935520814232672116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4935520814232672116(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4935520814232672116(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4935520814232672116);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9247765785938616612 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      40 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9247765785938616612(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 40} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9247765785938616612(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9247765785938616612(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9247765785938616612);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2693079858261458850 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1200 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2693079858261458850(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1200} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2693079858261458850(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2693079858261458850(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2693079858261458850);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3405475135743536612 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3405475135743536612(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3405475135743536612(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3405475135743536612(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3405475135743536612);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7336622147560601633 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7336622147560601633(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7336622147560601633(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7336622147560601633(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7336622147560601633);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2753031196419888036 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2753031196419888036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2753031196419888036(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2753031196419888036(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2753031196419888036);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14325097918240977038 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2016 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14325097918240977038(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2016} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14325097918240977038(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14325097918240977038(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14325097918240977038);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12951405361011888378 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1872 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12951405361011888378(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1872} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12951405361011888378(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12951405361011888378(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12951405361011888378);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3145359327511633830 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3145359327511633830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3145359327511633830(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3145359327511633830(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3145359327511633830);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9199178884131567376 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9199178884131567376(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9199178884131567376(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9199178884131567376(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9199178884131567376);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5924756572909850814 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5924756572909850814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5924756572909850814(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5924756572909850814(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5924756572909850814);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3254428102611447863 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1120 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3254428102611447863(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1120} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3254428102611447863(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3254428102611447863(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3254428102611447863);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3909402422437625349 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1600 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3909402422437625349(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1600} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3909402422437625349(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3909402422437625349(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3909402422437625349);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2085897917810651710 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2085897917810651710(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2085897917810651710(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2085897917810651710(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2085897917810651710);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11876094705122261116 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11876094705122261116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11876094705122261116(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11876094705122261116(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11876094705122261116);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7134172456111215490 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7134172456111215490(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7134172456111215490(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7134172456111215490(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7134172456111215490);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1777650841956822411 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1696 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1777650841956822411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1696} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1777650841956822411(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1777650841956822411(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1777650841956822411);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14422676426205775111 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14422676426205775111(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14422676426205775111(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14422676426205775111(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14422676426205775111);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15667268897409866557 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1200 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15667268897409866557(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1200} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15667268897409866557(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15667268897409866557(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15667268897409866557);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15719713987026732177 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1184 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15719713987026732177(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1184} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15719713987026732177(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15719713987026732177(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15719713987026732177);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6775251794091278733 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1184 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6775251794091278733(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1184} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6775251794091278733(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6775251794091278733(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6775251794091278733);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1634071652013627370 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1634071652013627370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1634071652013627370(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1634071652013627370(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1634071652013627370);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17038381344810214066 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1088 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17038381344810214066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1088} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17038381344810214066(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17038381344810214066(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17038381344810214066);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11780162273883578948 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2064 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11780162273883578948(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2064} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11780162273883578948(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11780162273883578948(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11780162273883578948);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__370245937712402787 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__370245937712402787(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__370245937712402787(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__370245937712402787(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__370245937712402787);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16233656173584574854 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16233656173584574854(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16233656173584574854(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16233656173584574854(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16233656173584574854);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2192875964661468509 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192875964661468509(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2192875964661468509(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2192875964661468509(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2192875964661468509);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9443962096651190569 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1888 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9443962096651190569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1888} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9443962096651190569(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9443962096651190569(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9443962096651190569);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4952995802142297655 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1472 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4952995802142297655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1472} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4952995802142297655(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4952995802142297655(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4952995802142297655);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14609288293154562996 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14609288293154562996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14609288293154562996(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14609288293154562996(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14609288293154562996);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3887671157963027890 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3887671157963027890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3887671157963027890(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3887671157963027890(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3887671157963027890);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__8271086534189809080 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8271086534189809080(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8271086534189809080(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8271086534189809080(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8271086534189809080);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7405892745759568552 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7405892745759568552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7405892745759568552(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7405892745759568552(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7405892745759568552);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1979262422386651619 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1979262422386651619(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1979262422386651619(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1979262422386651619(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1979262422386651619);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15276639300903055908 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15276639300903055908(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15276639300903055908(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15276639300903055908(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15276639300903055908);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17874453912388639610 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17874453912388639610(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17874453912388639610(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17874453912388639610(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17874453912388639610);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7589530368159172129 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7589530368159172129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7589530368159172129(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7589530368159172129(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7589530368159172129);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7092701620735340510 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7092701620735340510(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7092701620735340510(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7092701620735340510(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7092701620735340510);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__666421514218789921 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__666421514218789921(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__666421514218789921(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__666421514218789921(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__666421514218789921);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13921697624020255855 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1104 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13921697624020255855(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1104} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13921697624020255855(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13921697624020255855(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13921697624020255855);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8884970976558401067 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1504 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884970976558401067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1504} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8884970976558401067(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8884970976558401067(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8884970976558401067);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9974302228047877809 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9974302228047877809(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9974302228047877809(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9974302228047877809(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9974302228047877809);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1676322330266417960 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1676322330266417960(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1676322330266417960(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1676322330266417960(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1676322330266417960);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15966156339552418689 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15966156339552418689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15966156339552418689(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15966156339552418689(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15966156339552418689);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15928934563931947550 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15928934563931947550(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15928934563931947550(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15928934563931947550(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15928934563931947550);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9658389151292922427 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1680 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9658389151292922427(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1680} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9658389151292922427(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9658389151292922427(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9658389151292922427);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15932548867578434023 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15932548867578434023(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15932548867578434023(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15932548867578434023(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15932548867578434023);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15860334231651252884 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15860334231651252884(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15860334231651252884(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15860334231651252884(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15860334231651252884);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11773771973612341100 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1184 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11773771973612341100(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1184} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11773771973612341100(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11773771973612341100(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11773771973612341100);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7942257882978280502 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7942257882978280502(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7942257882978280502(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7942257882978280502(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7942257882978280502);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3771063693607098950 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      111 /* Input2 */, \
      111 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3771063693607098950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 111} /* Input2 */, 
      {"input[3]", 111} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3771063693607098950(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3771063693607098950(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3771063693607098950);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8225616590515531091 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8225616590515531091(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8225616590515531091(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8225616590515531091(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8225616590515531091);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1071859323191699231 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1071859323191699231(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1071859323191699231(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1071859323191699231(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1071859323191699231);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16176839292722443708 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1184 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16176839292722443708(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1184} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16176839292722443708(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16176839292722443708(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16176839292722443708);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14901914024326267927 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14901914024326267927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14901914024326267927(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14901914024326267927(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14901914024326267927);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1521466586923588412 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1104 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1521466586923588412(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1104} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1521466586923588412(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1521466586923588412(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1521466586923588412);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8099757491930504380 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8099757491930504380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8099757491930504380(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8099757491930504380(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8099757491930504380);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5389612321008999148 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5389612321008999148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5389612321008999148(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5389612321008999148(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5389612321008999148);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11123560122171190920 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1696 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11123560122171190920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1696} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11123560122171190920(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11123560122171190920(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11123560122171190920);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14523083182811040486 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14523083182811040486(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14523083182811040486(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14523083182811040486(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14523083182811040486);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7701250132073376478 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      912 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7701250132073376478(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 912} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7701250132073376478(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7701250132073376478(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7701250132073376478);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14950319592535475893 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      120 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      720 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14950319592535475893(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 120} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 720} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14950319592535475893(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14950319592535475893(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14950319592535475893);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1675340037455367400 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1675340037455367400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1675340037455367400(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1675340037455367400(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1675340037455367400);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10790494521622442692 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1504 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10790494521622442692(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1504} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10790494521622442692(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10790494521622442692(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10790494521622442692);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17417642037099398153 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1600 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17417642037099398153(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1600} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17417642037099398153(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17417642037099398153(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17417642037099398153);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15116361200006986569 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      80 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15116361200006986569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 80} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15116361200006986569(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15116361200006986569(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15116361200006986569);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1020868788925905787 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1020868788925905787(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1020868788925905787(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1020868788925905787(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1020868788925905787);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9771613107962993053 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1504 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9771613107962993053(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1504} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9771613107962993053(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9771613107962993053(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9771613107962993053);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15292953618773033313 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15292953618773033313(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15292953618773033313(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15292953618773033313(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15292953618773033313);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10736024067403003080 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10736024067403003080(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10736024067403003080(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10736024067403003080(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10736024067403003080);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12457236899657668027 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12457236899657668027(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12457236899657668027(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12457236899657668027(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12457236899657668027);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1710842492238772544 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1710842492238772544(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1710842492238772544(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1710842492238772544(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1710842492238772544);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7622883890680711962 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7622883890680711962(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7622883890680711962(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7622883890680711962(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7622883890680711962);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5936832729646089856 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5936832729646089856(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5936832729646089856(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5936832729646089856(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5936832729646089856);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3656839163748401343 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1120 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3656839163748401343(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1120} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3656839163748401343(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3656839163748401343(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3656839163748401343);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__603376117452541120 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__603376117452541120(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__603376117452541120(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__603376117452541120(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__603376117452541120);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17754282123995764606 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17754282123995764606(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17754282123995764606(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17754282123995764606(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17754282123995764606);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3672333716147590342 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1568 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3672333716147590342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1568} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3672333716147590342(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3672333716147590342(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3672333716147590342);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12710612273152522351 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12710612273152522351(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12710612273152522351(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12710612273152522351(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12710612273152522351);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17376956101855947760 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      7 /* FilterHeight */, \
      1 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17376956101855947760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17376956101855947760(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17376956101855947760(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17376956101855947760);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15491820704193361720 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1696 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15491820704193361720(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1696} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15491820704193361720(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15491820704193361720(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15491820704193361720);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15958414954997054403 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15958414954997054403(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15958414954997054403(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15958414954997054403(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15958414954997054403);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2848725006849396656 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1184 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2848725006849396656(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1184} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2848725006849396656(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2848725006849396656(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2848725006849396656);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1565165774330615972 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1565165774330615972(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1565165774330615972(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1565165774330615972(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1565165774330615972);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10671853544931053146 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10671853544931053146(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10671853544931053146(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10671853544931053146(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10671853544931053146);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__882146629087087309 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__882146629087087309(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__882146629087087309(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__882146629087087309(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__882146629087087309);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15269097253386281513 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15269097253386281513(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15269097253386281513(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15269097253386281513(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15269097253386281513);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16751741857869154785 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16751741857869154785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16751741857869154785(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16751741857869154785(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16751741857869154785);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4072013527306758339 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4072013527306758339(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4072013527306758339(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4072013527306758339(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4072013527306758339);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15379695058841947788 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      36 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      36 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15379695058841947788(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 36} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 36} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15379695058841947788(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15379695058841947788(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15379695058841947788);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2478798202053129035 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1376 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2478798202053129035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1376} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2478798202053129035(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2478798202053129035(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2478798202053129035);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12106142364571836323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12106142364571836323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12106142364571836323(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12106142364571836323(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12106142364571836323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1546711552435390125 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1546711552435390125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1546711552435390125(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1546711552435390125(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1546711552435390125);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17076799004147216697 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1504 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17076799004147216697(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1504} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17076799004147216697(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17076799004147216697(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17076799004147216697);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15008592940144755532 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1920 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15008592940144755532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1920} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15008592940144755532(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15008592940144755532(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15008592940144755532);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9397536157198670161 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      80 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      480 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9397536157198670161(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 80} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 480} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9397536157198670161(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9397536157198670161(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9397536157198670161);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12053247678704819343 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1392 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12053247678704819343(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1392} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12053247678704819343(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12053247678704819343(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12053247678704819343);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16759909306213648324 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1584 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16759909306213648324(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1584} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16759909306213648324(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16759909306213648324(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16759909306213648324);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10569769897882767999 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10569769897882767999(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10569769897882767999(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10569769897882767999(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10569769897882767999);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9896329680101106336 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9896329680101106336(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9896329680101106336(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9896329680101106336(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9896329680101106336);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1489612380794822448 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1489612380794822448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1489612380794822448(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1489612380794822448(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1489612380794822448);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4028266865786841061 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4028266865786841061(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4028266865786841061(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4028266865786841061(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4028266865786841061);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5587860659068285981 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2208 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5587860659068285981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2208} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5587860659068285981(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5587860659068285981(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5587860659068285981);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1562128695199185697 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1562128695199185697(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1562128695199185697(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1562128695199185697(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1562128695199185697);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14654948465140046723 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1584 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14654948465140046723(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1584} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14654948465140046723(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14654948465140046723(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14654948465140046723);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11985374742561606888 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11985374742561606888(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11985374742561606888(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11985374742561606888(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11985374742561606888);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16939527994611671930 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16939527994611671930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16939527994611671930(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16939527994611671930(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16939527994611671930);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12751824035240112839 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1008 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12751824035240112839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1008} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12751824035240112839(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12751824035240112839(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12751824035240112839);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13242169811032131244 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      8 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13242169811032131244(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13242169811032131244(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13242169811032131244(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13242169811032131244);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11859114871413628443 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1600 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11859114871413628443(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1600} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11859114871413628443(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11859114871413628443(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11859114871413628443);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3022015487775404950 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1792 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3022015487775404950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1792} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3022015487775404950(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3022015487775404950(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3022015487775404950);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3696737989371859713 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3696737989371859713(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3696737989371859713(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3696737989371859713(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3696737989371859713);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1403111733034636715 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      12 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1403111733034636715(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 12} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1403111733034636715(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1403111733034636715(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1403111733034636715);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__496769803992096099 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__496769803992096099(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__496769803992096099(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__496769803992096099(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__496769803992096099);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14337448415444596089 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14337448415444596089(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14337448415444596089(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14337448415444596089(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14337448415444596089);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__18406183419151128581 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1584 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18406183419151128581(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1584} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18406183419151128581(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18406183419151128581(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18406183419151128581);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13945484512375593928 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13945484512375593928(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13945484512375593928(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13945484512375593928(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13945484512375593928);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5201785321341848960 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5201785321341848960(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5201785321341848960(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5201785321341848960(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5201785321341848960);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11684575029283406381 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11684575029283406381(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11684575029283406381(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11684575029283406381(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11684575029283406381);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6857524923110559073 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6857524923110559073(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6857524923110559073(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6857524923110559073(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6857524923110559073);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4979120415295392849 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4979120415295392849(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4979120415295392849(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4979120415295392849(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4979120415295392849);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13142026807477108632 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13142026807477108632(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13142026807477108632(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13142026807477108632(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13142026807477108632);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11148102109731902865 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1488 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11148102109731902865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1488} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11148102109731902865(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11148102109731902865(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11148102109731902865);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11932655994289444306 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11932655994289444306(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11932655994289444306(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11932655994289444306(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11932655994289444306);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14185827533959183389 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14185827533959183389(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14185827533959183389(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14185827533959183389(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14185827533959183389);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__597802352355885604 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__597802352355885604(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__597802352355885604(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__597802352355885604(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__597802352355885604);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1509164778392698868 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1509164778392698868(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1509164778392698868(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1509164778392698868(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1509164778392698868);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8482698811146182277 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8482698811146182277(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8482698811146182277(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8482698811146182277(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8482698811146182277);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7563812796962210379 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7563812796962210379(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7563812796962210379(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7563812796962210379(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7563812796962210379);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16607036590801633798 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16607036590801633798(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16607036590801633798(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16607036590801633798(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16607036590801633798);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3149913402786362184 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3149913402786362184(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3149913402786362184(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3149913402786362184(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3149913402786362184);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10955846020583336571 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10955846020583336571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10955846020583336571(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10955846020583336571(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10955846020583336571);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1285099653572976280 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1888 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1285099653572976280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1888} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1285099653572976280(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1285099653572976280(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1285099653572976280);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__800071619049242428 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__800071619049242428(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__800071619049242428(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__800071619049242428(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__800071619049242428);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13769902418106801896 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1488 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13769902418106801896(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1488} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13769902418106801896(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13769902418106801896(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13769902418106801896);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17456224989006594939 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456224989006594939(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17456224989006594939(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17456224989006594939(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17456224989006594939);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5336202823276777724 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5336202823276777724(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5336202823276777724(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5336202823276777724(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5336202823276777724);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1699595385746163924 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1699595385746163924(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1699595385746163924(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1699595385746163924(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1699595385746163924);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6028076351197474639 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6028076351197474639(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6028076351197474639(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6028076351197474639(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6028076351197474639);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15699769796634179843 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15699769796634179843(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15699769796634179843(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15699769796634179843(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15699769796634179843);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14627640054720156423 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1872 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14627640054720156423(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1872} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14627640054720156423(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14627640054720156423(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14627640054720156423);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7329013293638302812 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7329013293638302812(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7329013293638302812(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7329013293638302812(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7329013293638302812);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14197525044280042844 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      816 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14197525044280042844(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 816} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14197525044280042844(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14197525044280042844(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14197525044280042844);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2394140345031557104 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2394140345031557104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2394140345031557104(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2394140345031557104(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2394140345031557104);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2084196655535569833 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1312 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2084196655535569833(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1312} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2084196655535569833(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2084196655535569833(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2084196655535569833);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4249664975107335695 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4249664975107335695(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4249664975107335695(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4249664975107335695(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4249664975107335695);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12443129452768933662 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1296 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12443129452768933662(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1296} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12443129452768933662(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12443129452768933662(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12443129452768933662);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5840280017414821124 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5840280017414821124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5840280017414821124(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5840280017414821124(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5840280017414821124);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8225773847475404932 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1216 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8225773847475404932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1216} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8225773847475404932(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8225773847475404932(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8225773847475404932);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4661207920625769981 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4661207920625769981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4661207920625769981(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4661207920625769981(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4661207920625769981);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5995369654622240346 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5995369654622240346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5995369654622240346(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5995369654622240346(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5995369654622240346);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__18200506672652092346 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18200506672652092346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18200506672652092346(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18200506672652092346(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18200506672652092346);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__1683496018596499508 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1683496018596499508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__1683496018596499508(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__1683496018596499508(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__1683496018596499508)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7117966749887302081 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      624 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7117966749887302081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 624} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7117966749887302081(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7117966749887302081(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7117966749887302081);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14585350761613081753 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14585350761613081753(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14585350761613081753(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14585350761613081753(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14585350761613081753);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2855234199113987992 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2855234199113987992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2855234199113987992(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2855234199113987992(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2855234199113987992);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15907613329436612776 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15907613329436612776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15907613329436612776(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15907613329436612776(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15907613329436612776);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__707088737775059182 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707088737775059182(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__707088737775059182(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__707088737775059182(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__707088737775059182);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11015183986437535597 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11015183986437535597(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11015183986437535597(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11015183986437535597(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11015183986437535597);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5706178967105261532 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5706178967105261532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5706178967105261532(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5706178967105261532(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5706178967105261532);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14063702995162793282 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14063702995162793282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14063702995162793282(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14063702995162793282(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14063702995162793282);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1980077735761224891 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2016 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1980077735761224891(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2016} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1980077735761224891(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1980077735761224891(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1980077735761224891);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15749921193702897876 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15749921193702897876(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15749921193702897876(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15749921193702897876(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15749921193702897876);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4638750011913646748 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4638750011913646748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4638750011913646748(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4638750011913646748(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4638750011913646748);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13412456502284697574 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1408 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13412456502284697574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1408} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13412456502284697574(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13412456502284697574(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13412456502284697574);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17213795763913424284 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17213795763913424284(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17213795763913424284(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17213795763913424284(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17213795763913424284);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11379014481374506821 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11379014481374506821(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11379014481374506821(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11379014481374506821(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11379014481374506821);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10356882087474698996 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10356882087474698996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10356882087474698996(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10356882087474698996(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10356882087474698996);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16059917806780559471 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16059917806780559471(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16059917806780559471(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16059917806780559471(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16059917806780559471);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5070961079720204993 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5070961079720204993(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5070961079720204993(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5070961079720204993(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5070961079720204993);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12454013449150329440 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12454013449150329440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12454013449150329440(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12454013449150329440(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12454013449150329440);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15042396690579570004 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      8 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15042396690579570004(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15042396690579570004(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15042396690579570004(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15042396690579570004);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16170400464930000276 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16170400464930000276(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16170400464930000276(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16170400464930000276(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16170400464930000276);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5055018582092196473 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5055018582092196473(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5055018582092196473(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5055018582092196473(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5055018582092196473);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5038217928992862 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5038217928992862(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5038217928992862(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5038217928992862(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5038217928992862);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16962269241204817890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1760 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16962269241204817890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1760} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16962269241204817890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16962269241204817890(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16962269241204817890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12433089478280496644 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12433089478280496644(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12433089478280496644(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12433089478280496644(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12433089478280496644);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5517839338317553098 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5517839338317553098(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5517839338317553098(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5517839338317553098(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5517839338317553098);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14875563138045850722 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14875563138045850722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14875563138045850722(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14875563138045850722(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14875563138045850722);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4209856892072190077 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4209856892072190077(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4209856892072190077(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4209856892072190077(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4209856892072190077);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7686096080792887889 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      71 /* Input2 */, \
      71 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7686096080792887889(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 71} /* Input2 */, 
      {"input[3]", 71} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7686096080792887889(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7686096080792887889(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7686096080792887889);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1032459654364357830 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1032459654364357830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1032459654364357830(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1032459654364357830(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1032459654364357830);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4738280583092677438 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4738280583092677438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4738280583092677438(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4738280583092677438(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4738280583092677438);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11599499732085733850 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11599499732085733850(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11599499732085733850(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11599499732085733850(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11599499732085733850);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10974055528144681273 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1152 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10974055528144681273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1152} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10974055528144681273(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10974055528144681273(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10974055528144681273);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5196272527345717184 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5196272527345717184(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5196272527345717184(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5196272527345717184(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5196272527345717184);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17771185594231005225 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1296 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17771185594231005225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1296} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17771185594231005225(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17771185594231005225(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17771185594231005225);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11469132194974178145 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11469132194974178145(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11469132194974178145(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11469132194974178145(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11469132194974178145);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16239955656745088457 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      528 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16239955656745088457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16239955656745088457(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16239955656745088457(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16239955656745088457);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12551294726579706817 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12551294726579706817(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12551294726579706817(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12551294726579706817(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12551294726579706817);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13529166907671197200 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13529166907671197200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13529166907671197200(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13529166907671197200(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13529166907671197200);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10549481638499073204 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10549481638499073204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10549481638499073204(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10549481638499073204(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10549481638499073204);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10573848680583193611 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10573848680583193611(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10573848680583193611(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10573848680583193611(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10573848680583193611);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9190477479438088043 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      36 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9190477479438088043(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 36} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9190477479438088043(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9190477479438088043(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9190477479438088043);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9068888135229569644 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9068888135229569644(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9068888135229569644(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9068888135229569644(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9068888135229569644);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2248074158922107272 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2248074158922107272(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2248074158922107272(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2248074158922107272(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2248074158922107272);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6789128059030530219 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1120 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6789128059030530219(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1120} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6789128059030530219(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6789128059030530219(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6789128059030530219);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14774257855729614792 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774257855729614792(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14774257855729614792(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14774257855729614792(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14774257855729614792);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18037387558451818657 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18037387558451818657(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18037387558451818657(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18037387558451818657(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18037387558451818657);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5078685221345927608 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1056 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5078685221345927608(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1056} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5078685221345927608(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5078685221345927608(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5078685221345927608);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11042097843420885826 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      299 /* Input2 */, \
      299 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11042097843420885826(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 299} /* Input2 */, 
      {"input[3]", 299} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11042097843420885826(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11042097843420885826(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11042097843420885826);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9614284770997866131 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      36 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9614284770997866131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 36} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9614284770997866131(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9614284770997866131(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9614284770997866131);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15391632453190816874 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15391632453190816874(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15391632453190816874(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15391632453190816874(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15391632453190816874);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13942274564422327109 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13942274564422327109(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13942274564422327109(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13942274564422327109(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13942274564422327109);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9742546307349084468 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9742546307349084468(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9742546307349084468(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9742546307349084468(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9742546307349084468);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2276594740433205871 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      80 /* Input1 */, \
      73 /* Input2 */, \
      73 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2276594740433205871(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 80} /* Input1 */, 
      {"input[2]", 73} /* Input2 */, 
      {"input[3]", 73} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2276594740433205871(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2276594740433205871(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2276594740433205871);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4832825698624872393 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4832825698624872393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4832825698624872393(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4832825698624872393(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4832825698624872393);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10216250167466496074 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      6 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      36 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10216250167466496074(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 6} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 36} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10216250167466496074(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10216250167466496074(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10216250167466496074);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9445461392392621464 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9445461392392621464(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9445461392392621464(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9445461392392621464(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9445461392392621464);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1415016069772552483 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      912 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1415016069772552483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 912} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1415016069772552483(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1415016069772552483(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1415016069772552483);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15996812711794329880 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1568 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15996812711794329880(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1568} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15996812711794329880(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15996812711794329880(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15996812711794329880);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13674041222657932961 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13674041222657932961(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13674041222657932961(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13674041222657932961(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13674041222657932961);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__6911464133923160919 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6911464133923160919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6911464133923160919(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6911464133923160919(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6911464133923160919);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15134001028903116878 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1568 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15134001028903116878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1568} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15134001028903116878(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15134001028903116878(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15134001028903116878);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18157822649457516180 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18157822649457516180(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18157822649457516180(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18157822649457516180(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18157822649457516180);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12934000316489358524 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12934000316489358524(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12934000316489358524(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12934000316489358524(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12934000316489358524);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17117509562386848792 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17117509562386848792(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17117509562386848792(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17117509562386848792(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17117509562386848792);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4880393912949925362 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4880393912949925362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4880393912949925362(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4880393912949925362(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4880393912949925362);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12576213745885515010 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12576213745885515010(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12576213745885515010(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12576213745885515010(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12576213745885515010);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7411629168173119890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      80 /* Input1 */, \
      73 /* Input2 */, \
      73 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7411629168173119890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 80} /* Input1 */, 
      {"input[2]", 73} /* Input2 */, 
      {"input[3]", 73} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7411629168173119890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7411629168173119890(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7411629168173119890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2611248643724057916 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1408 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2611248643724057916(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1408} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2611248643724057916(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2611248643724057916(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2611248643724057916);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15004828323256677811 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15004828323256677811(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15004828323256677811(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15004828323256677811(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15004828323256677811);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14505831141145247785 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14505831141145247785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14505831141145247785(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14505831141145247785(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14505831141145247785);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15985379043835092454 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1600 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15985379043835092454(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1600} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15985379043835092454(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15985379043835092454(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15985379043835092454);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6821890892987528153 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6821890892987528153(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6821890892987528153(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6821890892987528153(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6821890892987528153);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6919625131953586500 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6919625131953586500(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6919625131953586500(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6919625131953586500(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6919625131953586500);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6692240458440586685 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      4 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6692240458440586685(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 4} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6692240458440586685(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6692240458440586685(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6692240458440586685);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16041905850883005520 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16041905850883005520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16041905850883005520(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16041905850883005520(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16041905850883005520);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13815824236417463940 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13815824236417463940(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13815824236417463940(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13815824236417463940(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13815824236417463940);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2205158399739339057 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1200 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2205158399739339057(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1200} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2205158399739339057(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2205158399739339057(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2205158399739339057);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1350522387321241265 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1350522387321241265(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1350522387321241265(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1350522387321241265(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1350522387321241265);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2998328759991451104 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2998328759991451104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2998328759991451104(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2998328759991451104(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2998328759991451104);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2615624404182691672 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1392 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2615624404182691672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1392} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2615624404182691672(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2615624404182691672(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2615624404182691672);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7396702063963444746 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7396702063963444746(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7396702063963444746(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7396702063963444746(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7396702063963444746);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3589364966006668680 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3589364966006668680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3589364966006668680(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3589364966006668680(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3589364966006668680);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__638258412100153130 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1776 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__638258412100153130(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1776} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__638258412100153130(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__638258412100153130(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__638258412100153130);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7380149555332880386 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7380149555332880386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7380149555332880386(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7380149555332880386(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7380149555332880386);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4307250600110207711 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4307250600110207711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4307250600110207711(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4307250600110207711(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4307250600110207711);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11269755033714894744 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2064 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11269755033714894744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2064} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11269755033714894744(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11269755033714894744(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11269755033714894744);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5163404297775964086 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5163404297775964086(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5163404297775964086(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5163404297775964086(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5163404297775964086);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__6340789667059117375 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6340789667059117375(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6340789667059117375(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6340789667059117375(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6340789667059117375);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2812940008590052323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      128 /* FilterCount */, \
      7 /* FilterHeight */, \
      1 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2812940008590052323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2812940008590052323(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2812940008590052323(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2812940008590052323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14986037531144508552 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14986037531144508552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14986037531144508552(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14986037531144508552(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14986037531144508552);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7004293846750998693 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1376 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7004293846750998693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1376} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7004293846750998693(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7004293846750998693(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7004293846750998693);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16064325539048550234 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      12 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16064325539048550234(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 12} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16064325539048550234(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16064325539048550234(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16064325539048550234);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13166953091689387893 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13166953091689387893(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13166953091689387893(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13166953091689387893(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13166953091689387893);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13532319906833907666 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13532319906833907666(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13532319906833907666(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13532319906833907666(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13532319906833907666);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13651818648520036399 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13651818648520036399(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13651818648520036399(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13651818648520036399(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13651818648520036399);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6332530469930920776 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      8 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6332530469930920776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6332530469930920776(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6332530469930920776(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6332530469930920776);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__105019789268917252 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__105019789268917252(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__105019789268917252(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__105019789268917252(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__105019789268917252);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4626530156914982418 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4626530156914982418(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4626530156914982418(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4626530156914982418(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4626530156914982418);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15724722746869746236 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1104 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15724722746869746236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1104} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15724722746869746236(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15724722746869746236(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15724722746869746236);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16937432419754813512 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16937432419754813512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16937432419754813512(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16937432419754813512(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16937432419754813512);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12030235057058854069 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1216 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12030235057058854069(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1216} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12030235057058854069(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12030235057058854069(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12030235057058854069);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6493478788600485193 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6493478788600485193(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6493478788600485193(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6493478788600485193(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6493478788600485193);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5191817682501875173 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5191817682501875173(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5191817682501875173(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5191817682501875173(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5191817682501875173);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2776233561781792926 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2776233561781792926(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2776233561781792926(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2776233561781792926(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2776233561781792926);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7893952076445841730 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      9 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7893952076445841730(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 9} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7893952076445841730(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7893952076445841730(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7893952076445841730);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11275998808894148682 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11275998808894148682(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11275998808894148682(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11275998808894148682(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11275998808894148682);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16771145214891164205 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16771145214891164205(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16771145214891164205(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16771145214891164205(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16771145214891164205);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4083826431503492813 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4083826431503492813(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4083826431503492813(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4083826431503492813(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4083826431503492813);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18149128922960001255 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18149128922960001255(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18149128922960001255(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18149128922960001255(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18149128922960001255);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__18363653937187625568 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1504 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18363653937187625568(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1504} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18363653937187625568(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18363653937187625568(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18363653937187625568);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7765350016534492984 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7765350016534492984(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7765350016534492984(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7765350016534492984(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7765350016534492984);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1900342504083180923 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1900342504083180923(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1900342504083180923(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1900342504083180923(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1900342504083180923);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5882725750950726078 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5882725750950726078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5882725750950726078(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5882725750950726078(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5882725750950726078);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10607579075176672122 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10607579075176672122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10607579075176672122(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10607579075176672122(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10607579075176672122);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__487751976762359395 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1104 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__487751976762359395(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1104} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__487751976762359395(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__487751976762359395(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__487751976762359395);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7857782971449141348 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7857782971449141348(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7857782971449141348(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7857782971449141348(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7857782971449141348);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15724591344475382201 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2064 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15724591344475382201(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2064} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15724591344475382201(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15724591344475382201(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15724591344475382201);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13434023232066569919 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13434023232066569919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13434023232066569919(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13434023232066569919(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13434023232066569919);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__439727194961717665 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2112 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__439727194961717665(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2112} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__439727194961717665(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__439727194961717665(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__439727194961717665);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__645996522397770380 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__645996522397770380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__645996522397770380(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__645996522397770380(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__645996522397770380);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2686316368995923856 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1408 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2686316368995923856(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1408} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2686316368995923856(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2686316368995923856(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2686316368995923856);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10245682629386959879 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10245682629386959879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10245682629386959879(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10245682629386959879(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10245682629386959879);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11842421278146974234 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11842421278146974234(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11842421278146974234(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11842421278146974234(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11842421278146974234);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13198433036677472739 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13198433036677472739(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13198433036677472739(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13198433036677472739(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13198433036677472739);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6630641353807852653 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1408 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6630641353807852653(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1408} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6630641353807852653(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6630641353807852653(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6630641353807852653);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12529476141907450668 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      8 /* FilterHeight */, \
      8 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      8 /* StrideHeight */, \
      8 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12529476141907450668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_height", 8} /* FilterHeight */, 
      {"filter_width", 8} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 8} /* StrideHeight */, 
      {"stride_width", 8} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12529476141907450668(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12529476141907450668(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12529476141907450668);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1834789623297368696 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1834789623297368696(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1834789623297368696(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1834789623297368696(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1834789623297368696);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__809378892067624438 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      147 /* Input2 */, \
      147 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__809378892067624438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 147} /* Input2 */, 
      {"input[3]", 147} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__809378892067624438(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__809378892067624438(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__809378892067624438);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4934988246012302187 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4934988246012302187(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4934988246012302187(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4934988246012302187(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4934988246012302187);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7156158952106247415 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      448 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7156158952106247415(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 448} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7156158952106247415(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7156158952106247415(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7156158952106247415);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1462004000256677292 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1462004000256677292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1462004000256677292(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1462004000256677292(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1462004000256677292);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6866344306476460581 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6866344306476460581(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6866344306476460581(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6866344306476460581(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6866344306476460581);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7217716513688708932 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7217716513688708932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7217716513688708932(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7217716513688708932(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7217716513688708932);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12748230885521034209 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      4096 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12748230885521034209(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 4096} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12748230885521034209(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12748230885521034209(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12748230885521034209);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3762653656308007409 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3762653656308007409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3762653656308007409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3762653656308007409(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3762653656308007409);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7246228163424479760 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      2048 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7246228163424479760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7246228163424479760(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7246228163424479760(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7246228163424479760);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6107629980539097462 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6107629980539097462(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6107629980539097462(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6107629980539097462(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6107629980539097462);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8060011713818101476 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      1 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8060011713818101476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8060011713818101476(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8060011713818101476(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8060011713818101476);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13238531994458338280 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      9 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13238531994458338280(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 9} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13238531994458338280(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13238531994458338280(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13238531994458338280);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10331414536570065594 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1600 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10331414536570065594(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1600} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10331414536570065594(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10331414536570065594(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10331414536570065594);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15256513338936404224 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15256513338936404224(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15256513338936404224(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15256513338936404224(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15256513338936404224);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17365344489623910731 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17365344489623910731(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17365344489623910731(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17365344489623910731(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17365344489623910731);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14752673608290794990 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14752673608290794990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14752673608290794990(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14752673608290794990(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14752673608290794990);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12951341033229918595 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12951341033229918595(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12951341033229918595(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12951341033229918595(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12951341033229918595);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4798283810029484224 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4798283810029484224(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4798283810029484224(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4798283810029484224(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4798283810029484224);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13154751037888728222 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      7 /* FilterHeight */, \
      1 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13154751037888728222(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13154751037888728222(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13154751037888728222(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13154751037888728222);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2298294831976961527 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2298294831976961527(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2298294831976961527(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2298294831976961527(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2298294831976961527);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9176447228505451100 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2112 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9176447228505451100(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2112} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9176447228505451100(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9176447228505451100(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9176447228505451100);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3841554157244511932 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1680 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3841554157244511932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1680} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3841554157244511932(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3841554157244511932(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3841554157244511932);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11389318198671239433 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11389318198671239433(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11389318198671239433(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11389318198671239433(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11389318198671239433);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16769100223998948120 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16769100223998948120(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16769100223998948120(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16769100223998948120(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16769100223998948120);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8742214293780409251 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1248 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8742214293780409251(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1248} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8742214293780409251(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8742214293780409251(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8742214293780409251);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9495929893171576784 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1680 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9495929893171576784(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1680} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9495929893171576784(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9495929893171576784(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9495929893171576784);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15734886637520604937 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15734886637520604937(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15734886637520604937(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15734886637520604937(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15734886637520604937);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17476358920901797532 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17476358920901797532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17476358920901797532(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17476358920901797532(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17476358920901797532);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5327938172697605528 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1536 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5327938172697605528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1536} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5327938172697605528(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5327938172697605528(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5327938172697605528);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7566354804203082370 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7566354804203082370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7566354804203082370(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7566354804203082370(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7566354804203082370);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10634211173075228036 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10634211173075228036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10634211173075228036(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10634211173075228036(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10634211173075228036);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12380215451400402968 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12380215451400402968(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12380215451400402968(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12380215451400402968(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12380215451400402968);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3563200113864527485 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3563200113864527485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3563200113864527485(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3563200113864527485(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3563200113864527485);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13352820914223261533 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1344 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13352820914223261533(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1344} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13352820914223261533(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13352820914223261533(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13352820914223261533);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17809913046927674121 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17809913046927674121(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17809913046927674121(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17809913046927674121(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17809913046927674121);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1676681817911133222 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      7 /* FilterHeight */, \
      1 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1676681817911133222(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1676681817911133222(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1676681817911133222(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1676681817911133222);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11239313620362348852 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11239313620362348852(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11239313620362348852(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11239313620362348852(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11239313620362348852);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16762192949314553147 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16762192949314553147(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16762192949314553147(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16762192949314553147(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16762192949314553147);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9559788822104699896 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9559788822104699896(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9559788822104699896(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9559788822104699896(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9559788822104699896);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5297958803542825741 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5297958803542825741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5297958803542825741(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5297958803542825741(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5297958803542825741);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13955183217248810208 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13955183217248810208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13955183217248810208(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13955183217248810208(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13955183217248810208);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9927624397774172362 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9927624397774172362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9927624397774172362(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9927624397774172362(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9927624397774172362);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17379497147899793497 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17379497147899793497(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17379497147899793497(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17379497147899793497(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17379497147899793497);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__57823105745875735 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__57823105745875735(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__57823105745875735(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__57823105745875735(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__57823105745875735);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12350319103243834574 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12350319103243834574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12350319103243834574(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12350319103243834574(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12350319103243834574);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12887435403313130323 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1792 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12887435403313130323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1792} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12887435403313130323(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12887435403313130323(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12887435403313130323);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16543532568246489775 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16543532568246489775(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16543532568246489775(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16543532568246489775(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16543532568246489775);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10709040105338871845 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      149 /* Input2 */, \
      149 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10709040105338871845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 149} /* Input2 */, 
      {"input[3]", 149} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10709040105338871845(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10709040105338871845(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10709040105338871845);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2205217344475466986 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2112 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2205217344475466986(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2112} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2205217344475466986(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2205217344475466986(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2205217344475466986);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15015232157348521318 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15015232157348521318(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15015232157348521318(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15015232157348521318(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15015232157348521318);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6077626851797944062 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6077626851797944062(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6077626851797944062(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6077626851797944062(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6077626851797944062);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5477631195500157843 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      108 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5477631195500157843(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 108} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5477631195500157843(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5477631195500157843(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5477631195500157843);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10197127024869238557 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      816 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10197127024869238557(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 816} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10197127024869238557(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10197127024869238557(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10197127024869238557);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12807736408618390223 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12807736408618390223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12807736408618390223(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12807736408618390223(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12807736408618390223);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1346710784035622093 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1346710784035622093(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1346710784035622093(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1346710784035622093(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1346710784035622093);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12150853961934133545 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12150853961934133545(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12150853961934133545(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12150853961934133545(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12150853961934133545);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3572753541608992114 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3572753541608992114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3572753541608992114(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3572753541608992114(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3572753541608992114);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13599423829010152487 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1504 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13599423829010152487(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1504} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13599423829010152487(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13599423829010152487(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13599423829010152487);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11922718479321362634 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11922718479321362634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11922718479321362634(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11922718479321362634(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11922718479321362634);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14355914455106210075 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14355914455106210075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14355914455106210075(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14355914455106210075(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14355914455106210075);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8875966457825857890 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8875966457825857890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8875966457825857890(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8875966457825857890(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8875966457825857890);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14528494532646833920 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14528494532646833920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14528494532646833920(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14528494532646833920(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14528494532646833920);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15743405870119591481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1760 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15743405870119591481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1760} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15743405870119591481(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15743405870119591481(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15743405870119591481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9873234693987950003 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9873234693987950003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9873234693987950003(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9873234693987950003(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9873234693987950003);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11496626778774373770 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11496626778774373770(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11496626778774373770(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11496626778774373770(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11496626778774373770);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3052836410754229819 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      336 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3052836410754229819(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 336} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3052836410754229819(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3052836410754229819(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3052836410754229819);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__1486770262006256023 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1486770262006256023(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1486770262006256023(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1486770262006256023(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1486770262006256023);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2463082165989403564 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2463082165989403564(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2463082165989403564(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2463082165989403564(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2463082165989403564);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14476210761128950049 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      336 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14476210761128950049(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 336} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14476210761128950049(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14476210761128950049(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14476210761128950049);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13046057302864751514 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13046057302864751514(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13046057302864751514(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13046057302864751514(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13046057302864751514);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2118358741270397656 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2118358741270397656(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2118358741270397656(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2118358741270397656(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2118358741270397656);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16234175996121389953 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      640 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16234175996121389953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 640} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16234175996121389953(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16234175996121389953(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16234175996121389953);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3563754069956590945 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      7 /* StrideHeight */, \
      7 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3563754069956590945(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 7} /* StrideHeight */, 
      {"stride_width", 7} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3563754069956590945(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3563754069956590945(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3563754069956590945);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6556478031614833668 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6556478031614833668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6556478031614833668(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6556478031614833668(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6556478031614833668);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11839262173588210147 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11839262173588210147(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11839262173588210147(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11839262173588210147(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11839262173588210147);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10517238158247833829 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10517238158247833829(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10517238158247833829(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10517238158247833829(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10517238158247833829);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__9065981563073001356 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9065981563073001356(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__9065981563073001356(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9065981563073001356(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__9065981563073001356);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10508123989841937465 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1584 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10508123989841937465(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1584} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10508123989841937465(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10508123989841937465(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10508123989841937465);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17128355488928122700 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17128355488928122700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17128355488928122700(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17128355488928122700(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17128355488928122700);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14342948812766189139 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14342948812766189139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14342948812766189139(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14342948812766189139(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14342948812766189139);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__11334320192887469759 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      71 /* Input2 */, \
      71 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11334320192887469759(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 71} /* Input2 */, 
      {"input[3]", 71} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__11334320192887469759(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11334320192887469759(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__11334320192887469759);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8662226787338051577 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8662226787338051577(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8662226787338051577(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8662226787338051577(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8662226787338051577);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14866363091131050304 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14866363091131050304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14866363091131050304(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14866363091131050304(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14866363091131050304);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8144137880816647200 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8144137880816647200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8144137880816647200(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8144137880816647200(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8144137880816647200);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9992021574966326714 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9992021574966326714(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9992021574966326714(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9992021574966326714(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9992021574966326714);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14198810729541313914 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198810729541313914(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14198810729541313914(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14198810729541313914(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14198810729541313914);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7031907405517333362 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7031907405517333362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7031907405517333362(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7031907405517333362(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7031907405517333362);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14015820173284275693 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14015820173284275693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14015820173284275693(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14015820173284275693(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14015820173284275693);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14271446882209540006 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14271446882209540006(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14271446882209540006(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14271446882209540006(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14271446882209540006);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12019398994260271872 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12019398994260271872(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12019398994260271872(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12019398994260271872(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12019398994260271872);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10205512477118191808 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1376 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10205512477118191808(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1376} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10205512477118191808(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10205512477118191808(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10205512477118191808);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2091295603245123809 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      640 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2091295603245123809(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 640} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2091295603245123809(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2091295603245123809(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2091295603245123809);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13879537021420568996 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13879537021420568996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13879537021420568996(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13879537021420568996(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13879537021420568996);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10722771569765081061 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10722771569765081061(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10722771569765081061(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10722771569765081061(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10722771569765081061);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12297298470611486646 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12297298470611486646(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12297298470611486646(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12297298470611486646(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12297298470611486646);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17331384519160132407 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17331384519160132407(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17331384519160132407(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17331384519160132407(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17331384519160132407);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13174117471713896379 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13174117471713896379(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13174117471713896379(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13174117471713896379(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13174117471713896379);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16809821888720313460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16809821888720313460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16809821888720313460(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16809821888720313460(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16809821888720313460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6564969910806035469 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1968 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6564969910806035469(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1968} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6564969910806035469(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6564969910806035469(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6564969910806035469);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4266796538552569748 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4266796538552569748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4266796538552569748(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4266796538552569748(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4266796538552569748);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13530886937456969860 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13530886937456969860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13530886937456969860(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13530886937456969860(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13530886937456969860);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10544678633349832839 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10544678633349832839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10544678633349832839(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10544678633349832839(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10544678633349832839);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1248803682625969778 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1248803682625969778(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1248803682625969778(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1248803682625969778(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1248803682625969778);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13311987480569473728 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13311987480569473728(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13311987480569473728(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13311987480569473728(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13311987480569473728);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10091654921622258976 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10091654921622258976(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10091654921622258976(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10091654921622258976(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10091654921622258976);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11050461791991941589 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11050461791991941589(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11050461791991941589(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11050461791991941589(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11050461791991941589);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12020932041440111112 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12020932041440111112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12020932041440111112(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12020932041440111112(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12020932041440111112);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16667135170135599860 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16667135170135599860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16667135170135599860(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16667135170135599860(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16667135170135599860);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__600087448493412598 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__600087448493412598(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__600087448493412598(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__600087448493412598(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__600087448493412598);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3562380613897767410 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3562380613897767410(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3562380613897767410(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3562380613897767410(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3562380613897767410);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12234437852026018206 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12234437852026018206(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12234437852026018206(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12234437852026018206(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12234437852026018206);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9235052060981486417 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9235052060981486417(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9235052060981486417(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9235052060981486417(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9235052060981486417);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15006082659883886236 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15006082659883886236(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15006082659883886236(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15006082659883886236(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15006082659883886236);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2880194207456594259 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2880194207456594259(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2880194207456594259(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2880194207456594259(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2880194207456594259);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4896106523820196402 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4896106523820196402(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4896106523820196402(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4896106523820196402(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4896106523820196402);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2119235282636645561 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2119235282636645561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2119235282636645561(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2119235282636645561(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2119235282636645561);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2931331095126886153 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2931331095126886153(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2931331095126886153(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2931331095126886153(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2931331095126886153);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11473896854606807547 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1440 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11473896854606807547(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1440} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11473896854606807547(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11473896854606807547(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11473896854606807547);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13720866294880662403 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13720866294880662403(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13720866294880662403(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13720866294880662403(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13720866294880662403);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8111940442748732179 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      448 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8111940442748732179(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8111940442748732179(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8111940442748732179(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8111940442748732179);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15742926244289904836 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15742926244289904836(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15742926244289904836(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15742926244289904836(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15742926244289904836);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14914731307295567893 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      9 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14914731307295567893(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 9} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14914731307295567893(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14914731307295567893(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14914731307295567893);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12341470945507042227 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1568 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12341470945507042227(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1568} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12341470945507042227(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12341470945507042227(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12341470945507042227);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14019431782003440687 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14019431782003440687(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14019431782003440687(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14019431782003440687(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14019431782003440687);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16647849514114636135 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      160 /* FilterCount */, \
      7 /* FilterHeight */, \
      1 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16647849514114636135(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16647849514114636135(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16647849514114636135(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16647849514114636135);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3527058689327230606 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3527058689327230606(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3527058689327230606(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3527058689327230606(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3527058689327230606);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6371297522264561971 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6371297522264561971(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6371297522264561971(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6371297522264561971(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6371297522264561971);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5989410530870732230 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      336 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5989410530870732230(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 336} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5989410530870732230(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5989410530870732230(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5989410530870732230);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7482801105067105205 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7482801105067105205(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7482801105067105205(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7482801105067105205(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7482801105067105205);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14008170188353241040 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14008170188353241040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14008170188353241040(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14008170188353241040(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14008170188353241040);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__423025198693755616 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__423025198693755616(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__423025198693755616(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__423025198693755616(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__423025198693755616);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14899537853660490380 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14899537853660490380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14899537853660490380(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14899537853660490380(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14899537853660490380);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__674492672244867965 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__674492672244867965(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__674492672244867965(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__674492672244867965(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__674492672244867965);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1989022479953831979 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1989022479953831979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1989022479953831979(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1989022479953831979(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1989022479953831979);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16077932674993024158 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      147 /* Input2 */, \
      147 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16077932674993024158(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 147} /* Input2 */, 
      {"input[3]", 147} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16077932674993024158(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16077932674993024158(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16077932674993024158);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__16138682927320200989 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16138682927320200989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16138682927320200989(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16138682927320200989(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16138682927320200989);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14599381908517930524 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14599381908517930524(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14599381908517930524(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14599381908517930524(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14599381908517930524);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3107675361972651389 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3107675361972651389(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3107675361972651389(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3107675361972651389(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3107675361972651389);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15468648822912585962 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15468648822912585962(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15468648822912585962(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15468648822912585962(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15468648822912585962);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12202185523817648752 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12202185523817648752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12202185523817648752(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12202185523817648752(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12202185523817648752);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10181149336508041166 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10181149336508041166(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10181149336508041166(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10181149336508041166(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10181149336508041166);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7500701174357826959 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7500701174357826959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7500701174357826959(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7500701174357826959(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7500701174357826959);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18295947049782620165 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18295947049782620165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18295947049782620165(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18295947049782620165(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18295947049782620165);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10752078166708511055 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1088 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10752078166708511055(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1088} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10752078166708511055(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10752078166708511055(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10752078166708511055);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5434858216097664019 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      640 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5434858216097664019(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 640} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5434858216097664019(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5434858216097664019(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5434858216097664019);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11134419725065677631 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1872 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11134419725065677631(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1872} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11134419725065677631(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11134419725065677631(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11134419725065677631);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10150677553796185778 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10150677553796185778(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10150677553796185778(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10150677553796185778(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10150677553796185778);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8225972648645058652 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1792 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8225972648645058652(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1792} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8225972648645058652(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8225972648645058652(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8225972648645058652);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13419073279561116702 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13419073279561116702(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13419073279561116702(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13419073279561116702(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13419073279561116702);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__414237249745139475 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__414237249745139475(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__414237249745139475(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__414237249745139475(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__414237249745139475);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5299710532892114670 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5299710532892114670(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5299710532892114670(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5299710532892114670(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5299710532892114670);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6637406947132062815 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1200 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6637406947132062815(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1200} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6637406947132062815(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6637406947132062815(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6637406947132062815);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14722148784763971687 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14722148784763971687(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14722148784763971687(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14722148784763971687(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14722148784763971687);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9460807847525479483 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1472 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9460807847525479483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1472} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9460807847525479483(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9460807847525479483(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9460807847525479483);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__425579955387533679 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1296 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__425579955387533679(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1296} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__425579955387533679(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__425579955387533679(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__425579955387533679);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13562729916767335963 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      192 /* FilterCount */, \
      7 /* FilterHeight */, \
      1 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13562729916767335963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13562729916767335963(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13562729916767335963(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13562729916767335963);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3872170012053598854 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3872170012053598854(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3872170012053598854(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3872170012053598854(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3872170012053598854);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8033913525855062396 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8033913525855062396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8033913525855062396(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8033913525855062396(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8033913525855062396);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13080088861857006558 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      73 /* Input2 */, \
      73 /* Input3 */, \
      80 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13080088861857006558(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 73} /* Input2 */, 
      {"input[3]", 73} /* Input3 */, 
      {"filter_count", 80} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13080088861857006558(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13080088861857006558(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13080088861857006558);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__622407688626533532 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__622407688626533532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__622407688626533532(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__622407688626533532(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__622407688626533532);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3278204969648886444 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1392 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3278204969648886444(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1392} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3278204969648886444(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3278204969648886444(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3278204969648886444);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1499558185661905599 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      9 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1499558185661905599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 9} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1499558185661905599(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1499558185661905599(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1499558185661905599);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5024442708190210120 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5024442708190210120(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5024442708190210120(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5024442708190210120(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5024442708190210120);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10242159688275886499 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10242159688275886499(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10242159688275886499(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10242159688275886499(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10242159688275886499);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3221595725372625438 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3221595725372625438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3221595725372625438(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3221595725372625438(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3221595725372625438);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15394303368970180166 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15394303368970180166(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15394303368970180166(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15394303368970180166(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15394303368970180166);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5867116582666411197 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5867116582666411197(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5867116582666411197(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5867116582666411197(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5867116582666411197);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__331113643531188625 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__331113643531188625(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__331113643531188625(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__331113643531188625(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__331113643531188625);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7076477730262905374 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7076477730262905374(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7076477730262905374(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7076477730262905374(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7076477730262905374);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13713197379943978414 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13713197379943978414(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13713197379943978414(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13713197379943978414(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13713197379943978414);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14366846786243637041 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1632 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14366846786243637041(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1632} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14366846786243637041(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14366846786243637041(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14366846786243637041);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7227849320557992519 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      108 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7227849320557992519(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 108} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7227849320557992519(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7227849320557992519(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7227849320557992519);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5076636804315723257 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5076636804315723257(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5076636804315723257(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5076636804315723257(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5076636804315723257);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17676541682754307805 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17676541682754307805(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17676541682754307805(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17676541682754307805(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17676541682754307805);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8044857905478749963 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      80 /* Input1 */, \
      73 /* Input2 */, \
      73 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8044857905478749963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 80} /* Input1 */, 
      {"input[2]", 73} /* Input2 */, 
      {"input[3]", 73} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8044857905478749963(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8044857905478749963(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8044857905478749963);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17047840901191316952 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      4096 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17047840901191316952(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 4096} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17047840901191316952(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17047840901191316952(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17047840901191316952);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13723026971146825243 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2064 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13723026971146825243(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2064} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13723026971146825243(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13723026971146825243(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13723026971146825243);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12722839139948196032 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12722839139948196032(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12722839139948196032(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12722839139948196032(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12722839139948196032);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6160133305791982999 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6160133305791982999(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6160133305791982999(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6160133305791982999(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6160133305791982999);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13899581172428841288 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      4096 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13899581172428841288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 4096} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13899581172428841288(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13899581172428841288(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13899581172428841288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15689063511976032807 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15689063511976032807(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15689063511976032807(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15689063511976032807(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15689063511976032807);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13224476252622198292 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13224476252622198292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13224476252622198292(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13224476252622198292(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13224476252622198292);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15814114649943979864 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15814114649943979864(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15814114649943979864(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15814114649943979864(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15814114649943979864);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14994080449271845261 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14994080449271845261(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14994080449271845261(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14994080449271845261(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14994080449271845261);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1483025290631797442 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1483025290631797442(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1483025290631797442(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1483025290631797442(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1483025290631797442);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__441083520902505409 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__441083520902505409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__441083520902505409(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__441083520902505409(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__441083520902505409);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13257141158316352014 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13257141158316352014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13257141158316352014(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13257141158316352014(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13257141158316352014);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8248790197115216268 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8248790197115216268(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8248790197115216268(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8248790197115216268(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8248790197115216268);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4229167427036660175 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4229167427036660175(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4229167427036660175(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4229167427036660175(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4229167427036660175);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13784725694487356507 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13784725694487356507(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13784725694487356507(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13784725694487356507(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13784725694487356507);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15142885790439905075 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15142885790439905075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15142885790439905075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15142885790439905075(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15142885790439905075);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15683785741626336832 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15683785741626336832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15683785741626336832(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15683785741626336832(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15683785741626336832);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2071943396568080260 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2071943396568080260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2071943396568080260(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2071943396568080260(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2071943396568080260);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4084871147191120541 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4084871147191120541(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4084871147191120541(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4084871147191120541(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4084871147191120541);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9832223560663405476 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9832223560663405476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9832223560663405476(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9832223560663405476(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9832223560663405476);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15011790065878607047 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      7 /* StrideHeight */, \
      7 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15011790065878607047(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 7} /* StrideHeight */, 
      {"stride_width", 7} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15011790065878607047(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15011790065878607047(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15011790065878607047);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5689136256785344447 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5689136256785344447(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5689136256785344447(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5689136256785344447(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5689136256785344447);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8756939132577846295 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2064 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8756939132577846295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2064} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8756939132577846295(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8756939132577846295(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8756939132577846295);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4543553446484106685 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4543553446484106685(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4543553446484106685(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4543553446484106685(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4543553446484106685);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16496011774814361056 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16496011774814361056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16496011774814361056(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16496011774814361056(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16496011774814361056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7914988433698560319 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7914988433698560319(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7914988433698560319(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7914988433698560319(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7914988433698560319);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5581317009934287372 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5581317009934287372(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5581317009934287372(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5581317009934287372(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5581317009934287372);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1026038479905173181 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1026038479905173181(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1026038479905173181(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1026038479905173181(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1026038479905173181);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__37171157204981775 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__37171157204981775(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__37171157204981775(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__37171157204981775(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__37171157204981775);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9827136600492892895 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9827136600492892895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9827136600492892895(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9827136600492892895(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9827136600492892895);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11989489554956496739 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      147 /* Input2 */, \
      147 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11989489554956496739(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 147} /* Input2 */, 
      {"input[3]", 147} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11989489554956496739(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11989489554956496739(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11989489554956496739);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6736953351282253505 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1408 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6736953351282253505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1408} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6736953351282253505(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6736953351282253505(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6736953351282253505);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11549808731100468016 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      2048 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11549808731100468016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__11549808731100468016(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11549808731100468016(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11549808731100468016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7501813554998219560 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7501813554998219560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7501813554998219560(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7501813554998219560(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7501813554998219560);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9568232307032040719 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9568232307032040719(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9568232307032040719(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9568232307032040719(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9568232307032040719);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14137624741563830131 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14137624741563830131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14137624741563830131(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14137624741563830131(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14137624741563830131);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2731054126573516064 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2731054126573516064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2731054126573516064(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2731054126573516064(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2731054126573516064);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17736298376766034247 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1600 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17736298376766034247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1600} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17736298376766034247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17736298376766034247(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17736298376766034247);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17072266971247038036 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17072266971247038036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17072266971247038036(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17072266971247038036(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17072266971247038036);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6967813040566314627 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1392 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6967813040566314627(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1392} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6967813040566314627(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6967813040566314627(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6967813040566314627);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8610402141851086144 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8610402141851086144(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8610402141851086144(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8610402141851086144(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8610402141851086144);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15313031836662691977 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15313031836662691977(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15313031836662691977(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15313031836662691977(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15313031836662691977);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4908717512326296760 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      640 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4908717512326296760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 640} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4908717512326296760(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4908717512326296760(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4908717512326296760);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10310154202305335416 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10310154202305335416(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10310154202305335416(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10310154202305335416(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10310154202305335416);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4845355855513984771 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4845355855513984771(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4845355855513984771(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4845355855513984771(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4845355855513984771);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17139948750849467352 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      149 /* Input2 */, \
      149 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17139948750849467352(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 149} /* Input2 */, 
      {"input[3]", 149} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17139948750849467352(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17139948750849467352(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17139948750849467352);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15845470720272220315 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15845470720272220315(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15845470720272220315(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15845470720272220315(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15845470720272220315);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12097583185774157741 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12097583185774157741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12097583185774157741(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12097583185774157741(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12097583185774157741);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12744315454272851197 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12744315454272851197(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12744315454272851197(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12744315454272851197(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12744315454272851197);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6568453581797242618 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6568453581797242618(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6568453581797242618(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6568453581797242618(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6568453581797242618);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2421005628194196348 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2421005628194196348(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2421005628194196348(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2421005628194196348(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2421005628194196348);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9296558241212514918 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9296558241212514918(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9296558241212514918(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9296558241212514918(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9296558241212514918);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16459999249456734661 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16459999249456734661(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16459999249456734661(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16459999249456734661(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16459999249456734661);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4340261633556198506 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4340261633556198506(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4340261633556198506(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4340261633556198506(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4340261633556198506);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15855626140751389957 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15855626140751389957(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15855626140751389957(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15855626140751389957(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15855626140751389957);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11754113178204507380 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11754113178204507380(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11754113178204507380(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11754113178204507380(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11754113178204507380);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15315491998383925247 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15315491998383925247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15315491998383925247(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15315491998383925247(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15315491998383925247);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17648933169894811644 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17648933169894811644(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17648933169894811644(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17648933169894811644(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17648933169894811644);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7499448824476257088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7499448824476257088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7499448824476257088(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7499448824476257088(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7499448824476257088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7531115499352961548 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7531115499352961548(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7531115499352961548(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7531115499352961548(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7531115499352961548);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12187762831196497563 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12187762831196497563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12187762831196497563(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12187762831196497563(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12187762831196497563);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13318207178132846432 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13318207178132846432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13318207178132846432(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13318207178132846432(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13318207178132846432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5850813150040238730 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5850813150040238730(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5850813150040238730(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5850813150040238730(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5850813150040238730);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13086315527576725396 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1664 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13086315527576725396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1664} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13086315527576725396(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13086315527576725396(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13086315527576725396);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2205440395692087368 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2205440395692087368(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2205440395692087368(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2205440395692087368(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2205440395692087368);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8586913195090260839 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8586913195090260839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8586913195090260839(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8586913195090260839(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8586913195090260839);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17434151279470724312 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17434151279470724312(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17434151279470724312(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17434151279470724312(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17434151279470724312);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14534496754821017914 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1008 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14534496754821017914(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1008} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14534496754821017914(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14534496754821017914(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14534496754821017914);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15548861751218908137 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15548861751218908137(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15548861751218908137(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15548861751218908137(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15548861751218908137);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14637249745955287879 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1856 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14637249745955287879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1856} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14637249745955287879(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14637249745955287879(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14637249745955287879);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__606717011945862570 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__606717011945862570(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__606717011945862570(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__606717011945862570(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__606717011945862570);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__808024856955279484 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      12 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__808024856955279484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 12} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__808024856955279484(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__808024856955279484(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__808024856955279484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1313558539887983695 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      12 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1313558539887983695(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 12} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1313558539887983695(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1313558539887983695(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1313558539887983695);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7077923150236739398 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2016 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7077923150236739398(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2016} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7077923150236739398(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7077923150236739398(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7077923150236739398);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15526959676967046805 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15526959676967046805(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15526959676967046805(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15526959676967046805(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15526959676967046805);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__596368439858839517 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__596368439858839517(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__596368439858839517(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__596368439858839517(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__596368439858839517);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2323532505342604222 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2323532505342604222(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2323532505342604222(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2323532505342604222(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2323532505342604222);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11910064264272455416 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      4096 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11910064264272455416(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11910064264272455416(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11910064264272455416(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11910064264272455416);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12657085463027415672 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12657085463027415672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12657085463027415672(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12657085463027415672(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12657085463027415672);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9823854110283855681 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9823854110283855681(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9823854110283855681(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9823854110283855681(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9823854110283855681);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3851510227113415062 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      13 /* FilterHeight */, \
      13 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      13 /* StrideHeight */, \
      13 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3851510227113415062(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 13} /* FilterHeight */, 
      {"filter_width", 13} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 13} /* StrideHeight */, 
      {"stride_width", 13} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3851510227113415062(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3851510227113415062(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3851510227113415062);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13459627919190144525 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13459627919190144525(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13459627919190144525(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13459627919190144525(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13459627919190144525);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9478221153260023160 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1280 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9478221153260023160(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9478221153260023160(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9478221153260023160(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9478221153260023160);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4781823289424042608 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      768 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4781823289424042608(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 768} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4781823289424042608(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4781823289424042608(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4781823289424042608);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8248358656564336139 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      147 /* Input2 */, \
      147 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8248358656564336139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 147} /* Input2 */, 
      {"input[3]", 147} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8248358656564336139(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8248358656564336139(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8248358656564336139);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__417577929590577342 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      288 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__417577929590577342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__417577929590577342(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__417577929590577342(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__417577929590577342);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10618501468636316729 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10618501468636316729(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10618501468636316729(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10618501468636316729(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10618501468636316729);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11719800018046485350 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11719800018046485350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11719800018046485350(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11719800018046485350(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11719800018046485350);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12265704035826505140 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1312 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12265704035826505140(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1312} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12265704035826505140(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12265704035826505140(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12265704035826505140);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15396523622963683840 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15396523622963683840(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15396523622963683840(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15396523622963683840(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15396523622963683840);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9925207221182610102 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9925207221182610102(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9925207221182610102(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9925207221182610102(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9925207221182610102);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6425340545385085332 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6425340545385085332(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6425340545385085332(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6425340545385085332(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6425340545385085332);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2129298645672895541 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1760 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2129298645672895541(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1760} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2129298645672895541(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2129298645672895541(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2129298645672895541);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16075574116940779285 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16075574116940779285(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16075574116940779285(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16075574116940779285(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16075574116940779285);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16417546300217078550 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16417546300217078550(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16417546300217078550(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16417546300217078550(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16417546300217078550);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4613726804193174770 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      2048 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4613726804193174770(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4613726804193174770(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4613726804193174770(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4613726804193174770);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17645719245387504445 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1376 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17645719245387504445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1376} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17645719245387504445(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17645719245387504445(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17645719245387504445);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4480752760346835926 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4480752760346835926(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4480752760346835926(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4480752760346835926(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4480752760346835926);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16202560284848502052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      720 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16202560284848502052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 720} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16202560284848502052(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16202560284848502052(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16202560284848502052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10810372940747101025 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10810372940747101025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10810372940747101025(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10810372940747101025(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10810372940747101025);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4537404082918616159 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4537404082918616159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4537404082918616159(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4537404082918616159(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4537404082918616159);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__153663427360567917 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      336 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__153663427360567917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 336} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__153663427360567917(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__153663427360567917(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__153663427360567917);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17822035160060175002 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      72 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17822035160060175002(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17822035160060175002(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17822035160060175002(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17822035160060175002);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__59242718069621398 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__59242718069621398(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__59242718069621398(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__59242718069621398(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__59242718069621398);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12180253774506432192 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1216 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12180253774506432192(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1216} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12180253774506432192(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12180253774506432192(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12180253774506432192);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11123801595164829933 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11123801595164829933(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11123801595164829933(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11123801595164829933(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11123801595164829933);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6703783309711552108 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6703783309711552108(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6703783309711552108(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6703783309711552108(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6703783309711552108);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2304271864329817566 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2304271864329817566(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2304271864329817566(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2304271864329817566(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2304271864329817566);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1434691179756413356 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      71 /* Input2 */, \
      71 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1434691179756413356(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 71} /* Input2 */, 
      {"input[3]", 71} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1434691179756413356(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1434691179756413356(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1434691179756413356);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16365198353737444112 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16365198353737444112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16365198353737444112(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16365198353737444112(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16365198353737444112);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14089871974094927344 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14089871974094927344(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14089871974094927344(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14089871974094927344(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14089871974094927344);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4723193818150347386 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1472 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4723193818150347386(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1472} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4723193818150347386(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4723193818150347386(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4723193818150347386);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16103017379022566748 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16103017379022566748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16103017379022566748(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16103017379022566748(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16103017379022566748);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15387574408052714265 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15387574408052714265(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15387574408052714265(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15387574408052714265(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15387574408052714265);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17750503832108127109 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      17 /* Input2 */, \
      17 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17750503832108127109(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 17} /* Input2 */, 
      {"input[3]", 17} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17750503832108127109(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17750503832108127109(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17750503832108127109);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7240710914189597759 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7240710914189597759(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7240710914189597759(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7240710914189597759(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7240710914189597759);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13065022046993846668 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13065022046993846668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13065022046993846668(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13065022046993846668(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13065022046993846668);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7628325774716342909 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7628325774716342909(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7628325774716342909(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7628325774716342909(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7628325774716342909);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9326794732856187194 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9326794732856187194(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9326794732856187194(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9326794732856187194(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9326794732856187194);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2939854510014131177 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2939854510014131177(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2939854510014131177(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2939854510014131177(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2939854510014131177);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12079561884060808082 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12079561884060808082(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12079561884060808082(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12079561884060808082(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12079561884060808082);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2914163351669959031 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2914163351669959031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2914163351669959031(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2914163351669959031(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2914163351669959031);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15171553967062596581 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15171553967062596581(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15171553967062596581(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15171553967062596581(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15171553967062596581);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10785983336979729481 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10785983336979729481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10785983336979729481(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10785983336979729481(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10785983336979729481);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2357288313297023876 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2357288313297023876(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2357288313297023876(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2357288313297023876(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2357288313297023876);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7954777005361665225 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7954777005361665225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7954777005361665225(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7954777005361665225(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7954777005361665225);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9411919828180450074 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9411919828180450074(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9411919828180450074(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9411919828180450074(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9411919828180450074);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8410000652644826695 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1088 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8410000652644826695(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1088} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8410000652644826695(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8410000652644826695(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8410000652644826695);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3625274544054981026 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3625274544054981026(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3625274544054981026(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3625274544054981026(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3625274544054981026);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1544261076047136874 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1544261076047136874(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1544261076047136874(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1544261076047136874(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1544261076047136874);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4138370962038409330 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4138370962038409330(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4138370962038409330(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4138370962038409330(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4138370962038409330);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2478288300801029383 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2478288300801029383(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2478288300801029383(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2478288300801029383(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2478288300801029383);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9688476155452308411 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      18 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9688476155452308411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 18} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9688476155452308411(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9688476155452308411(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9688476155452308411);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13259184459371853016 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      72 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13259184459371853016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13259184459371853016(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13259184459371853016(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13259184459371853016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14419196758892008638 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1488 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14419196758892008638(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1488} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14419196758892008638(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14419196758892008638(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14419196758892008638);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17033196460873540448 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17033196460873540448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17033196460873540448(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17033196460873540448(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17033196460873540448);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4696290271097574221 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696290271097574221(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4696290271097574221(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4696290271097574221(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4696290271097574221);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3535533849340283416 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3535533849340283416(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3535533849340283416(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3535533849340283416(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3535533849340283416);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16828335381540498957 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      624 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16828335381540498957(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 624} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16828335381540498957(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16828335381540498957(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16828335381540498957);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9823769025470233603 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9823769025470233603(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9823769025470233603(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9823769025470233603(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9823769025470233603);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7327354943150225492 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1312 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7327354943150225492(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1312} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7327354943150225492(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7327354943150225492(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7327354943150225492);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7145150604031583642 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      147 /* Input2 */, \
      147 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7145150604031583642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 147} /* Input2 */, 
      {"input[3]", 147} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7145150604031583642(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7145150604031583642(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7145150604031583642);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13178319380765341566 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13178319380765341566(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13178319380765341566(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13178319380765341566(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13178319380765341566);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3577798301824861421 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3577798301824861421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3577798301824861421(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3577798301824861421(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3577798301824861421);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14558568953831343461 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      192 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14558568953831343461(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14558568953831343461(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14558568953831343461(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14558568953831343461);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12211695601850906190 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12211695601850906190(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12211695601850906190(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12211695601850906190(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12211695601850906190);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8162448977746276693 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8162448977746276693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8162448977746276693(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8162448977746276693(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8162448977746276693);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8981449901695847058 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1296 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8981449901695847058(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1296} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8981449901695847058(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8981449901695847058(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8981449901695847058);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15910683502213656088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      12 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      72 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15910683502213656088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 12} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15910683502213656088(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15910683502213656088(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15910683502213656088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9275218669438756617 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      35 /* Input2 */, \
      35 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9275218669438756617(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 35} /* Input2 */, 
      {"input[3]", 35} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9275218669438756617(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9275218669438756617(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9275218669438756617);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4612760431586940327 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4612760431586940327(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4612760431586940327(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4612760431586940327(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4612760431586940327);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17246877776004447493 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17246877776004447493(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17246877776004447493(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17246877776004447493(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17246877776004447493);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7959690220675533344 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7959690220675533344(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7959690220675533344(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7959690220675533344(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7959690220675533344);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__440167489243966221 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      18 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__440167489243966221(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 18} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__440167489243966221(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__440167489243966221(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__440167489243966221);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5252770798660126480 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5252770798660126480(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5252770798660126480(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5252770798660126480(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5252770798660126480);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10426094413818969172 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      432 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10426094413818969172(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 432} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10426094413818969172(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10426094413818969172(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10426094413818969172);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14162977960331748277 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14162977960331748277(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14162977960331748277(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14162977960331748277(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14162977960331748277);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17828776474980990970 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17828776474980990970(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17828776474980990970(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17828776474980990970(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17828776474980990970);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17448981829756011545 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1792 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      896 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17448981829756011545(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1792} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 896} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17448981829756011545(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17448981829756011545(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17448981829756011545);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2276692942859640950 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2276692942859640950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2276692942859640950(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2276692942859640950(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2276692942859640950);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8979492120968882461 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      20 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8979492120968882461(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 20} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8979492120968882461(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8979492120968882461(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8979492120968882461);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15866929383509315059 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15866929383509315059(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15866929383509315059(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15866929383509315059(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15866929383509315059);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15216383097006114167 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1728 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15216383097006114167(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1728} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15216383097006114167(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15216383097006114167(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15216383097006114167);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16232146466308320366 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      960 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16232146466308320366(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16232146466308320366(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16232146466308320366(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16232146466308320366);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15330655383340362349 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15330655383340362349(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15330655383340362349(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15330655383340362349(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15330655383340362349);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13065779802559829705 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13065779802559829705(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13065779802559829705(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13065779802559829705(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13065779802559829705);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15168154444919291041 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1824 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15168154444919291041(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1824} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15168154444919291041(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15168154444919291041(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15168154444919291041);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12484665295550318180 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12484665295550318180(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12484665295550318180(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12484665295550318180(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12484665295550318180);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17507745244678338274 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      20 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17507745244678338274(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 20} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17507745244678338274(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17507745244678338274(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17507745244678338274);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5544685119370468992 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      40 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5544685119370468992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 40} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5544685119370468992(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5544685119370468992(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5544685119370468992);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10750743325756521199 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10750743325756521199(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10750743325756521199(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10750743325756521199(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10750743325756521199);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17325529067544730029 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17325529067544730029(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17325529067544730029(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17325529067544730029(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17325529067544730029);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13205390749594328111 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13205390749594328111(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13205390749594328111(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13205390749594328111(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13205390749594328111);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13696244408724310288 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      768 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13696244408724310288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 768} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13696244408724310288(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13696244408724310288(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13696244408724310288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11967648175298973453 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11967648175298973453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11967648175298973453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11967648175298973453(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11967648175298973453);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5943214772090975273 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5943214772090975273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5943214772090975273(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5943214772090975273(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5943214772090975273);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__502815713504843483 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__502815713504843483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__502815713504843483(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__502815713504843483(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__502815713504843483);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3226322458527964807 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3226322458527964807(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3226322458527964807(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3226322458527964807(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3226322458527964807);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8874844159994524181 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      18 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      108 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8874844159994524181(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 18} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 108} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8874844159994524181(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8874844159994524181(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8874844159994524181);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14375319225648204690 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14375319225648204690(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14375319225648204690(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14375319225648204690(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14375319225648204690);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18257078052157023884 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      12 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      72 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18257078052157023884(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 12} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__18257078052157023884(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18257078052157023884(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18257078052157023884);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5696251274193968624 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5696251274193968624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5696251274193968624(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5696251274193968624(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5696251274193968624);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8014417976204370663 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8014417976204370663(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8014417976204370663(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8014417976204370663(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8014417976204370663);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18128532808771075226 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18128532808771075226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18128532808771075226(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18128532808771075226(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18128532808771075226);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5443448987615417804 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      8 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5443448987615417804(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5443448987615417804(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5443448987615417804(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5443448987615417804);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12164215875530711754 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12164215875530711754(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12164215875530711754(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12164215875530711754(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12164215875530711754);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5549500541650218212 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5549500541650218212(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5549500541650218212(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5549500541650218212(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5549500541650218212);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2944876120490401118 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2944876120490401118(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2944876120490401118(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2944876120490401118(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2944876120490401118);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7141632445889023622 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7141632445889023622(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7141632445889023622(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7141632445889023622(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7141632445889023622);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13311194258563558299 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13311194258563558299(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13311194258563558299(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13311194258563558299(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13311194258563558299);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6666533467022183919 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      109 /* Input2 */, \
      109 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6666533467022183919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 109} /* Input2 */, 
      {"input[3]", 109} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6666533467022183919(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6666533467022183919(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6666533467022183919);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6090784841686042340 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6090784841686042340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6090784841686042340(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6090784841686042340(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6090784841686042340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5663207673403071907 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5663207673403071907(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5663207673403071907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5663207673403071907(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5663207673403071907);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7779559248863036770 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7779559248863036770(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7779559248863036770(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7779559248863036770(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7779559248863036770);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3623942370146997516 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      6 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3623942370146997516(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 6} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3623942370146997516(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3623942370146997516(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3623942370146997516);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9233020267433586281 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      108 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      18 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9233020267433586281(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 108} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 18} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9233020267433586281(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9233020267433586281(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9233020267433586281);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2340762940945850003 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2340762940945850003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2340762940945850003(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2340762940945850003(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2340762940945850003);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15522882769665300540 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15522882769665300540(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15522882769665300540(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15522882769665300540(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15522882769665300540);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5491101578766633466 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5491101578766633466(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5491101578766633466(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5491101578766633466(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5491101578766633466);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15104446776061516717 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15104446776061516717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15104446776061516717(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15104446776061516717(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15104446776061516717);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10084395800631985603 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      12 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10084395800631985603(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 12} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10084395800631985603(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10084395800631985603(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10084395800631985603);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13313323786257950797 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13313323786257950797(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13313323786257950797(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13313323786257950797(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13313323786257950797);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2534308654578610304 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2534308654578610304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2534308654578610304(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2534308654578610304(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2534308654578610304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11883969767927802124 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11883969767927802124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11883969767927802124(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11883969767927802124(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11883969767927802124);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17389971359365626073 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17389971359365626073(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17389971359365626073(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17389971359365626073(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17389971359365626073);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12045070817912702025 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12045070817912702025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12045070817912702025(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12045070817912702025(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12045070817912702025);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3586019466413101569 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3586019466413101569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3586019466413101569(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3586019466413101569(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3586019466413101569);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4981831102478632286 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4981831102478632286(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4981831102478632286(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4981831102478632286(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4981831102478632286);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7447421507642897963 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447421507642897963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7447421507642897963(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447421507642897963(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7447421507642897963);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9908569458119962660 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      40 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9908569458119962660(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 40} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9908569458119962660(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9908569458119962660(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9908569458119962660);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16149271699815373497 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16149271699815373497(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16149271699815373497(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16149271699815373497(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16149271699815373497);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12597907554463955642 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12597907554463955642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12597907554463955642(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12597907554463955642(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12597907554463955642);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3938979781692596669 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1000 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3938979781692596669(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3938979781692596669(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3938979781692596669(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3938979781692596669);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14371000834190504025 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14371000834190504025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14371000834190504025(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14371000834190504025(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14371000834190504025);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16694992638543472482 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      432 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16694992638543472482(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 432} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16694992638543472482(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16694992638543472482(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16694992638543472482);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15670376539343981295 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15670376539343981295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15670376539343981295(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15670376539343981295(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15670376539343981295);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9281860769796879688 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9281860769796879688(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9281860769796879688(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9281860769796879688(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9281860769796879688);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5007775793133490616 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007775793133490616(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5007775793133490616(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5007775793133490616(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5007775793133490616);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2012018453321400472 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2012018453321400472(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2012018453321400472(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2012018453321400472(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2012018453321400472);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17515076074105379015 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17515076074105379015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17515076074105379015(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17515076074105379015(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17515076074105379015);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3792644319728678656 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3792644319728678656(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3792644319728678656(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3792644319728678656(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3792644319728678656);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__989713833842464696 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__989713833842464696(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__989713833842464696(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__989713833842464696(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__989713833842464696);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15562217950158823464 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15562217950158823464(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15562217950158823464(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15562217950158823464(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15562217950158823464);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7039980933607194806 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7039980933607194806(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7039980933607194806(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7039980933607194806(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7039980933607194806);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3187309595795173401 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      1024 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3187309595795173401(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3187309595795173401(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3187309595795173401(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3187309595795173401);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3974547583610548048 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3974547583610548048(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3974547583610548048(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3974547583610548048(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3974547583610548048);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14182411628950662214 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14182411628950662214(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14182411628950662214(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14182411628950662214(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14182411628950662214);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14925589119662214235 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14925589119662214235(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14925589119662214235(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14925589119662214235(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14925589119662214235);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7778571967242391344 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7778571967242391344(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7778571967242391344(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7778571967242391344(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7778571967242391344);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10662469995143420673 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10662469995143420673(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10662469995143420673(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10662469995143420673(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10662469995143420673);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3188011300048535240 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      36 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      36 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3188011300048535240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 36} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 36} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3188011300048535240(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3188011300048535240(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3188011300048535240);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__296690761189427687 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__296690761189427687(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__296690761189427687(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__296690761189427687(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__296690761189427687);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16658677085937170472 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16658677085937170472(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16658677085937170472(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16658677085937170472(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16658677085937170472);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4939545238849452845 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4939545238849452845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4939545238849452845(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4939545238849452845(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4939545238849452845);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4331893052486952024 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      108 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      108 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4331893052486952024(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 108} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 108} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4331893052486952024(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4331893052486952024(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4331893052486952024);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17882700980748442311 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17882700980748442311(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17882700980748442311(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17882700980748442311(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17882700980748442311);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7122758126801054678 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7122758126801054678(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7122758126801054678(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7122758126801054678(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7122758126801054678);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13392991490486223449 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13392991490486223449(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13392991490486223449(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13392991490486223449(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13392991490486223449);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3811087104897770251 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3811087104897770251(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3811087104897770251(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3811087104897770251(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3811087104897770251);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9925116873508173956 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      960 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9925116873508173956(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9925116873508173956(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9925116873508173956(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9925116873508173956);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16735701435092210172 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16735701435092210172(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16735701435092210172(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16735701435092210172(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16735701435092210172);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9052281940631152835 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      240 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9052281940631152835(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 240} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9052281940631152835(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9052281940631152835(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9052281940631152835);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13503799931959122238 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13503799931959122238(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13503799931959122238(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13503799931959122238(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13503799931959122238);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3885962383150796202 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3885962383150796202(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3885962383150796202(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3885962383150796202(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3885962383150796202);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17532985028507808243 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17532985028507808243(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17532985028507808243(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17532985028507808243(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17532985028507808243);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6972583601582869512 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      72 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6972583601582869512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 72} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6972583601582869512(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6972583601582869512(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6972583601582869512);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8366544652782068769 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8366544652782068769(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__8366544652782068769(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8366544652782068769(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8366544652782068769);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14936487325183926962 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      768 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14936487325183926962(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 768} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14936487325183926962(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14936487325183926962(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14936487325183926962);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__16153125620885642063 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16153125620885642063(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16153125620885642063(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16153125620885642063(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16153125620885642063);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15882326085141159078 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15882326085141159078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15882326085141159078(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15882326085141159078(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15882326085141159078);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15568370600021120284 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15568370600021120284(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15568370600021120284(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15568370600021120284(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15568370600021120284);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14208460617485946264 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14208460617485946264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14208460617485946264(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14208460617485946264(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14208460617485946264);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9793739077925882250 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9793739077925882250(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9793739077925882250(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9793739077925882250(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9793739077925882250);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13140542823843147188 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      48 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13140542823843147188(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13140542823843147188(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13140542823843147188(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13140542823843147188);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11474524794321926863 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11474524794321926863(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11474524794321926863(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11474524794321926863(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11474524794321926863);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9919959566443951756 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9919959566443951756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9919959566443951756(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9919959566443951756(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9919959566443951756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8860777492167918 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      72 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8860777492167918(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 72} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8860777492167918(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8860777492167918(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8860777492167918);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5561141865160730634 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      128 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5561141865160730634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5561141865160730634(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5561141865160730634(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5561141865160730634);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1708646782582365930 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1708646782582365930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1708646782582365930(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1708646782582365930(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1708646782582365930);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5858212667684497482 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5858212667684497482(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__5858212667684497482(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5858212667684497482(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5858212667684497482);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16539071025559086760 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16539071025559086760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16539071025559086760(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16539071025559086760(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16539071025559086760);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3920256039579990482 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      240 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3920256039579990482(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 240} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3920256039579990482(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3920256039579990482(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3920256039579990482);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7629482495244045328 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      432 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      432 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629482495244045328(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 432} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 432} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7629482495244045328(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7629482495244045328(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7629482495244045328);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9000683424816984483 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      80 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9000683424816984483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 80} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9000683424816984483(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9000683424816984483(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9000683424816984483);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1021602081838789783 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1021602081838789783(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1021602081838789783(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1021602081838789783(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1021602081838789783);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1163272705418250256 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1163272705418250256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__1163272705418250256(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1163272705418250256(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1163272705418250256);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9726574793010415444 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9726574793010415444(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__9726574793010415444(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9726574793010415444(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9726574793010415444);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13523641482786068644 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      11 /* FilterHeight */, \
      11 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      4 /* StrideHeight */, \
      4 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13523641482786068644(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 11} /* FilterHeight */, 
      {"filter_width", 11} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 4} /* StrideHeight */, 
      {"stride_width", 4} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13523641482786068644(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13523641482786068644(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13523641482786068644);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14216819000372859932 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      108 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      108 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14216819000372859932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 108} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 108} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14216819000372859932(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14216819000372859932(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14216819000372859932);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12366190515321012126 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      36 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      6 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12366190515321012126(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 36} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 6} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12366190515321012126(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12366190515321012126(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12366190515321012126);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17217764050835232266 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17217764050835232266(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__17217764050835232266(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17217764050835232266(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17217764050835232266);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3456111488838921018 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3456111488838921018(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3456111488838921018(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3456111488838921018(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3456111488838921018);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16027118894421804980 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16027118894421804980(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16027118894421804980(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16027118894421804980(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16027118894421804980);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7953264791601923404 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      480 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      480 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7953264791601923404(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 480} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__7953264791601923404(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7953264791601923404(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7953264791601923404);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6644060887170783920 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      512 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6644060887170783920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6644060887170783920(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6644060887170783920(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6644060887170783920);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12348196838199057189 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12348196838199057189(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12348196838199057189(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12348196838199057189(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12348196838199057189);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10851480491162443014 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      48 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10851480491162443014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10851480491162443014(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10851480491162443014(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10851480491162443014);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14494370494978145170 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14494370494978145170(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14494370494978145170(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14494370494978145170(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14494370494978145170);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2749685840515745048 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2749685840515745048(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2749685840515745048(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2749685840515745048(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2749685840515745048);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3980427910169517950 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3980427910169517950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3980427910169517950(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3980427910169517950(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3980427910169517950);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12550187426430292576 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12550187426430292576(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12550187426430292576(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12550187426430292576(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12550187426430292576);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__576064252303054611 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__576064252303054611(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__576064252303054611(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__576064252303054611(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__576064252303054611);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12315077854520141597 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12315077854520141597(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12315077854520141597(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12315077854520141597(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12315077854520141597);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14238553039211798481 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14238553039211798481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14238553039211798481(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14238553039211798481(state);
}

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14238553039211798481);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16080348846801147165 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16080348846801147165(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16080348846801147165(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16080348846801147165(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16080348846801147165);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3174747360709916156 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      16 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3174747360709916156(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__3174747360709916156(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3174747360709916156(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3174747360709916156);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6009922520128791412 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      8 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6009922520128791412(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__6009922520128791412(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6009922520128791412(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6009922520128791412);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12243366872685694845 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      720 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      120 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12243366872685694845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 720} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 120} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__12243366872685694845(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12243366872685694845(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12243366872685694845);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4984509809432022450 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4984509809432022450(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4984509809432022450(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4984509809432022450(state);
}

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4984509809432022450);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10564231944719462929 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      384 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10564231944719462929(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__10564231944719462929(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10564231944719462929(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10564231944719462929);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4609485941120841820 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4609485941120841820(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__4609485941120841820(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4609485941120841820(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4609485941120841820);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16888409698524634895 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16888409698524634895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__16888409698524634895(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16888409698524634895(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16888409698524634895);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__8295337808632242623 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8295337808632242623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8295337808632242623(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8295337808632242623(state);
  }
  
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8295337808632242623);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15038759040368705828 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15038759040368705828(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__15038759040368705828(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15038759040368705828(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15038759040368705828);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13073444609464679516 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073444609464679516(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__13073444609464679516(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073444609464679516(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13073444609464679516);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2681606764310862223 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      32 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2681606764310862223(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__2681606764310862223(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2681606764310862223(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2681606764310862223);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14018705387472696917 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      2 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14018705387472696917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 2} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}

  
  template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
  static void LAYER_CUDNN_CONV_FWD_FLOAT32__14018705387472696917(benchmark::State& state) {
    
      LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
    
    BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14018705387472696917(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14018705387472696917);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
